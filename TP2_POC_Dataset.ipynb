{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Computer Vision**\n",
        "\n",
        "Term Project : ResNet-50 from Scratch Trained on the POC Dataset\n",
        "\n",
        "202111941 PARK SEJIN"
      ],
      "metadata": {
        "id": "d2KYcBxnSyGh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2LxmEmI1QPk",
        "outputId": "d1b730d4-1c15-4c25-fab7-9580a754d70d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Setting up GPU environment...\n",
            "CuPy already installed\n",
            "Using GPU: <CUDA Device 0>\n",
            "GPU Memory: 15.8 GB\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# Colab + GPU (CuPy)\n",
        "# ============================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Setting up GPU environment...\")\n",
        "try:\n",
        "    import cupy as cp\n",
        "    print(\"CuPy already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing CuPy...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"cupy-cuda12x\", \"-q\"])\n",
        "    import cupy as cp\n",
        "    print(\"CuPy installed!\")\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# GPU\n",
        "USE_GPU = True\n",
        "if USE_GPU:\n",
        "    try:\n",
        "        xp = cp\n",
        "        print(f\"Using GPU: {cp.cuda.Device()}\")\n",
        "        print(f\"GPU Memory: {cp.cuda.Device().mem_info[1] / 1e9:.1f} GB\\n\")\n",
        "    except:\n",
        "        xp = np\n",
        "        USE_GPU = False\n",
        "        print(\"GPU not available, falling back to CPU\\n\")\n",
        "else:\n",
        "    xp = np\n",
        "    print(\"Using CPU (NumPy)\\n\")\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/POC_Dataset'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Utility Func\n",
        "# ============================================\n",
        "\n",
        "def to_cpu(x):\n",
        "    if USE_GPU:\n",
        "        return cp.asnumpy(x)\n",
        "    return x\n",
        "\n",
        "def to_gpu(x):\n",
        "    if USE_GPU:\n",
        "        return cp.asarray(x)\n",
        "    return x\n",
        "\n",
        "def im2col(x, fh, fw, stride, pad):\n",
        "    n, c, h, w = x.shape\n",
        "    oh = (h + 2*pad - fh) // stride + 1\n",
        "    ow = (w + 2*pad - fw) // stride + 1\n",
        "\n",
        "    x_pad = xp.pad(x, [(0,0), (0,0), (pad,pad), (pad,pad)], 'constant')\n",
        "    col = xp.zeros((n, c, fh, fw, oh, ow), dtype=x.dtype)\n",
        "\n",
        "    for y in range(fh):\n",
        "        y_max = y + stride*oh\n",
        "        for x_pos in range(fw):\n",
        "            x_max = x_pos + stride*ow\n",
        "            col[:, :, y, x_pos, :, :] = x_pad[:, :, y:y_max:stride, x_pos:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(n*oh*ow, -1)\n",
        "    return col\n",
        "\n",
        "def col2im(col, x_shape, fh, fw, stride, pad):\n",
        "    n, c, h, w = x_shape\n",
        "    oh = (h + 2*pad - fh) // stride + 1\n",
        "    ow = (w + 2*pad - fw) // stride + 1\n",
        "\n",
        "    col = col.reshape(n, oh, ow, c, fh, fw).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "    x = xp.zeros((n, c, h + 2*pad + stride - 1, w + 2*pad + stride - 1), dtype=col.dtype)\n",
        "    for y in range(fh):\n",
        "        y_max = y + stride*oh\n",
        "        for x_pos in range(fw):\n",
        "            x_max = x_pos + stride*ow\n",
        "            x[:, :, y:y_max:stride, x_pos:x_max:stride] += col[:, :, y, x_pos, :, :]\n",
        "\n",
        "    return x[:, :, pad:h+pad, pad:w+pad]"
      ],
      "metadata": {
        "id": "kEykTGsg14Oe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Layer class\n",
        "# ============================================\n",
        "\n",
        "class Conv2D:\n",
        "    def __init__(self, in_ch, out_ch, ksize, stride=1, pad=0):\n",
        "        self.W = xp.random.randn(out_ch, in_ch, ksize, ksize).astype(xp.float32) * xp.sqrt(2.0 / (in_ch * ksize * ksize))\n",
        "        self.b = xp.zeros(out_ch, dtype=xp.float32)\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "    def forward(self, x):\n",
        "        fh, fw = self.W.shape[2:]\n",
        "        n, c, h, w = x.shape\n",
        "        oh = (h + 2*self.pad - fh) // self.stride + 1\n",
        "        ow = (w + 2*self.pad - fw) // self.stride + 1\n",
        "\n",
        "        col = im2col(x, fh, fw, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(self.W.shape[0], -1).T\n",
        "\n",
        "        out = xp.dot(col, col_W) + self.b\n",
        "        out = out.reshape(n, oh, ow, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        fh, fw = self.W.shape[2:]\n",
        "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, self.W.shape[0])\n",
        "\n",
        "        self.db = xp.sum(dout, axis=0)\n",
        "        self.dW = xp.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(self.W.shape)\n",
        "\n",
        "        dcol = xp.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, fh, fw, self.stride, self.pad)\n",
        "        return dx\n",
        "\n",
        "class BatchNorm:\n",
        "    def __init__(self, num_features, momentum=0.9):\n",
        "        self.gamma = xp.ones(num_features, dtype=xp.float32)\n",
        "        self.beta = xp.zeros(num_features, dtype=xp.float32)\n",
        "        self.momentum = momentum\n",
        "        self.running_mean = xp.zeros(num_features, dtype=xp.float32)\n",
        "        self.running_var = xp.ones(num_features, dtype=xp.float32)\n",
        "\n",
        "    def forward(self, x, train=True):\n",
        "        if x.ndim == 4:\n",
        "            n, c, h, w = x.shape\n",
        "            x = x.transpose(0, 2, 3, 1).reshape(-1, c)\n",
        "\n",
        "        if train:\n",
        "            mean = x.mean(axis=0)\n",
        "            var = x.var(axis=0)\n",
        "            std = xp.sqrt(var + 1e-5)\n",
        "            xc = x - mean\n",
        "            xn = xc / std\n",
        "\n",
        "            self.batch_mean = mean\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "\n",
        "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
        "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
        "        else:\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / xp.sqrt(self.running_var + 1e-5)\n",
        "\n",
        "        out = self.gamma * xn + self.beta\n",
        "        out = out.reshape(n, h, w, c).transpose(0, 3, 1, 2)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if dout.ndim == 4:\n",
        "            n, c, h, w = dout.shape\n",
        "            dout = dout.transpose(0, 2, 3, 1).reshape(-1, c)\n",
        "\n",
        "        self.dbeta = dout.sum(axis=0)\n",
        "        self.dgamma = xp.sum(self.xn * dout, axis=0)\n",
        "\n",
        "        dxn = self.gamma * dout\n",
        "        dxc = dxn / self.std\n",
        "        dstd = -xp.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "        dxc += (2.0 / len(dout)) * self.xc * dvar\n",
        "        dmean = xp.sum(dxc, axis=0)\n",
        "        dx = dxc - dmean / len(dout)\n",
        "\n",
        "        dx = dx.reshape(n, h, w, c).transpose(0, 3, 1, 2)\n",
        "        return dx\n",
        "\n",
        "class ReLU:\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        return dout\n",
        "\n",
        "class MaxPooling:\n",
        "    def __init__(self, ksize=2, stride=2):\n",
        "        self.ksize = ksize\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        n, c, h, w = x.shape\n",
        "        oh = (h - self.ksize) // self.stride + 1\n",
        "        ow = (w - self.ksize) // self.stride + 1\n",
        "\n",
        "        col = im2col(x, self.ksize, self.ksize, self.stride, 0)\n",
        "        col = col.reshape(-1, self.ksize*self.ksize)\n",
        "\n",
        "        self.arg_max = xp.argmax(col, axis=1)\n",
        "        out = xp.max(col, axis=1)\n",
        "        out = out.reshape(n, oh, ow, c).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "\n",
        "        pool_size = self.ksize * self.ksize\n",
        "        dmax = xp.zeros((dout.size, pool_size), dtype=dout.dtype)\n",
        "        dmax[xp.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
        "\n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.ksize, self.ksize, self.stride, 0)\n",
        "        return dx\n",
        "\n",
        "class GlobalAvgPooling:\n",
        "    def forward(self, x):\n",
        "        self.x_shape = x.shape\n",
        "        return xp.mean(x, axis=(2, 3))\n",
        "\n",
        "    def backward(self, dout):\n",
        "        n, c, h, w = self.x_shape\n",
        "        dx = dout.reshape(n, c, 1, 1) / (h * w)\n",
        "        return xp.tile(dx, (1, 1, h, w))\n",
        "\n",
        "class FullyConnected:\n",
        "    def __init__(self, in_size, out_size):\n",
        "        self.W = xp.random.randn(in_size, out_size).astype(xp.float32) * xp.sqrt(2.0 / in_size)\n",
        "        self.b = xp.zeros(out_size, dtype=xp.float32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return xp.dot(x, self.W) + self.b\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = xp.dot(dout, self.W.T)\n",
        "        self.dW = xp.dot(self.x.T, dout)\n",
        "        self.db = xp.sum(dout, axis=0)\n",
        "        return dx"
      ],
      "metadata": {
        "id": "T4qkG42Z19gu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ResNet Bottleneck Block\n",
        "# ============================================\n",
        "\n",
        "class BottleneckBlock:\n",
        "    def __init__(self, in_ch, mid_ch, out_ch, stride=1, downsample=False):\n",
        "        self.conv1 = Conv2D(in_ch, mid_ch, 1, stride=1, pad=0)\n",
        "        self.bn1 = BatchNorm(mid_ch)\n",
        "        self.relu1 = ReLU()\n",
        "\n",
        "        self.conv2 = Conv2D(mid_ch, mid_ch, 3, stride=stride, pad=1)\n",
        "        self.bn2 = BatchNorm(mid_ch)\n",
        "        self.relu2 = ReLU()\n",
        "\n",
        "        self.conv3 = Conv2D(mid_ch, out_ch, 1, stride=1, pad=0)\n",
        "        self.bn3 = BatchNorm(out_ch)\n",
        "\n",
        "        self.downsample = None\n",
        "        if downsample:\n",
        "            self.downsample = [\n",
        "                Conv2D(in_ch, out_ch, 1, stride=stride, pad=0),\n",
        "                BatchNorm(out_ch)\n",
        "            ]\n",
        "\n",
        "        self.relu3 = ReLU()\n",
        "\n",
        "    def forward(self, x, train=True):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1.forward(x)\n",
        "        out = self.bn1.forward(out, train)\n",
        "        out = self.relu1.forward(out)\n",
        "\n",
        "        out = self.conv2.forward(out)\n",
        "        out = self.bn2.forward(out, train)\n",
        "        out = self.relu2.forward(out)\n",
        "\n",
        "        out = self.conv3.forward(out)\n",
        "        out = self.bn3.forward(out, train)\n",
        "\n",
        "        if self.downsample:\n",
        "            identity = self.downsample[0].forward(x)\n",
        "            identity = self.downsample[1].forward(identity, train)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu3.forward(out)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = self.relu3.backward(dout)\n",
        "        didentity = dout\n",
        "\n",
        "        dout = self.bn3.backward(dout)\n",
        "        dout = self.conv3.backward(dout)\n",
        "\n",
        "        dout = self.relu2.backward(dout)\n",
        "        dout = self.bn2.backward(dout)\n",
        "        dout = self.conv2.backward(dout)\n",
        "\n",
        "        dout = self.relu1.backward(dout)\n",
        "        dout = self.bn1.backward(dout)\n",
        "        dout = self.conv1.backward(dout)\n",
        "\n",
        "        if self.downsample:\n",
        "            didentity = self.downsample[1].backward(didentity)\n",
        "            didentity = self.downsample[0].backward(didentity)\n",
        "\n",
        "        return dout + didentity"
      ],
      "metadata": {
        "id": "qRUf805A2AXj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ResNet-50\n",
        "# ============================================\n",
        "\n",
        "class ResNet50:\n",
        "    def __init__(self, num_classes=4):\n",
        "        print(\"Initializing ResNet-50...\")\n",
        "        self.conv1 = Conv2D(3, 64, 7, stride=2, pad=3)\n",
        "        self.bn1 = BatchNorm(64)\n",
        "        self.relu = ReLU()\n",
        "        self.maxpool = MaxPooling(ksize=3, stride=2)\n",
        "\n",
        "        self.layer1 = [\n",
        "            BottleneckBlock(64, 64, 256, stride=1, downsample=True),\n",
        "            BottleneckBlock(256, 64, 256),\n",
        "            BottleneckBlock(256, 64, 256)\n",
        "        ]\n",
        "\n",
        "        self.layer2 = [\n",
        "            BottleneckBlock(256, 128, 512, stride=2, downsample=True),\n",
        "            BottleneckBlock(512, 128, 512),\n",
        "            BottleneckBlock(512, 128, 512),\n",
        "            BottleneckBlock(512, 128, 512)\n",
        "        ]\n",
        "\n",
        "        self.layer3 = [\n",
        "            BottleneckBlock(512, 256, 1024, stride=2, downsample=True),\n",
        "            BottleneckBlock(1024, 256, 1024),\n",
        "            BottleneckBlock(1024, 256, 1024),\n",
        "            BottleneckBlock(1024, 256, 1024),\n",
        "            BottleneckBlock(1024, 256, 1024),\n",
        "            BottleneckBlock(1024, 256, 1024)\n",
        "        ]\n",
        "\n",
        "        self.layer4 = [\n",
        "            BottleneckBlock(1024, 512, 2048, stride=2, downsample=True),\n",
        "            BottleneckBlock(2048, 512, 2048),\n",
        "            BottleneckBlock(2048, 512, 2048)\n",
        "        ]\n",
        "\n",
        "        self.avgpool = GlobalAvgPooling()\n",
        "        self.fc = FullyConnected(2048, num_classes)\n",
        "        print(\"ResNet-50 initialized\")\n",
        "\n",
        "    def forward(self, x, train=True):\n",
        "        x = self.conv1.forward(x)\n",
        "        x = self.bn1.forward(x, train)\n",
        "        x = self.relu.forward(x)\n",
        "        x = self.maxpool.forward(x)\n",
        "\n",
        "        for block in self.layer1:\n",
        "            x = block.forward(x, train)\n",
        "        for block in self.layer2:\n",
        "            x = block.forward(x, train)\n",
        "        for block in self.layer3:\n",
        "            x = block.forward(x, train)\n",
        "        for block in self.layer4:\n",
        "            x = block.forward(x, train)\n",
        "\n",
        "        x = self.avgpool.forward(x)\n",
        "        x = self.fc.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = self.fc.backward(dout)\n",
        "        dout = self.avgpool.backward(dout)\n",
        "\n",
        "        for block in reversed(self.layer4):\n",
        "            dout = block.backward(dout)\n",
        "        for block in reversed(self.layer3):\n",
        "            dout = block.backward(dout)\n",
        "        for block in reversed(self.layer2):\n",
        "            dout = block.backward(dout)\n",
        "        for block in reversed(self.layer1):\n",
        "            dout = block.backward(dout)\n",
        "\n",
        "        dout = self.maxpool.backward(dout)\n",
        "        dout = self.relu.backward(dout)\n",
        "        dout = self.bn1.backward(dout)\n",
        "        dout = self.conv1.backward(dout)\n",
        "        return dout\n",
        "\n",
        "    def get_params(self):\n",
        "        params = []\n",
        "        params.append((self.conv1, 'conv'))\n",
        "        params.append((self.bn1, 'bn'))\n",
        "\n",
        "        for block in self.layer1 + self.layer2 + self.layer3 + self.layer4:\n",
        "            params.append((block.conv1, 'conv'))\n",
        "            params.append((block.bn1, 'bn'))\n",
        "            params.append((block.conv2, 'conv'))\n",
        "            params.append((block.bn2, 'bn'))\n",
        "            params.append((block.conv3, 'conv'))\n",
        "            params.append((block.bn3, 'bn'))\n",
        "            if block.downsample:\n",
        "                params.append((block.downsample[0], 'conv'))\n",
        "                params.append((block.downsample[1], 'bn'))\n",
        "\n",
        "        params.append((self.fc, 'fc'))\n",
        "        return params"
      ],
      "metadata": {
        "id": "lyY1ih6n2Cns"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Loss func and Optimizer\n",
        "# ============================================\n",
        "\n",
        "class CrossEntropyLoss:\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = self.softmax(x)\n",
        "        return self.cross_entropy(self.y, t)\n",
        "\n",
        "    def backward(self):\n",
        "        batch_size = self.t.shape[0]\n",
        "        dx = self.y.copy()\n",
        "        dx[xp.arange(batch_size), self.t] -= 1\n",
        "        dx = dx / batch_size\n",
        "        return dx\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        x = x - xp.max(x, axis=1, keepdims=True)\n",
        "        return xp.exp(x) / xp.sum(xp.exp(x), axis=1, keepdims=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def cross_entropy(y, t):\n",
        "        batch_size = y.shape[0]\n",
        "        return float(-xp.sum(xp.log(y[xp.arange(batch_size), t] + 1e-7)) / batch_size)\n",
        "\n",
        "class SGD:\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = {}\n",
        "\n",
        "    def update(self, params):\n",
        "        for i, (layer, layer_type) in enumerate(params):\n",
        "            key = f'layer_{i}'\n",
        "\n",
        "            if layer_type == 'conv':\n",
        "                if key not in self.v:\n",
        "                    self.v[key] = {'W': xp.zeros_like(layer.W), 'b': xp.zeros_like(layer.b)}\n",
        "\n",
        "                self.v[key]['W'] = self.momentum * self.v[key]['W'] - self.lr * layer.dW\n",
        "                self.v[key]['b'] = self.momentum * self.v[key]['b'] - self.lr * layer.db\n",
        "\n",
        "                layer.W += self.v[key]['W']\n",
        "                layer.b += self.v[key]['b']\n",
        "\n",
        "            elif layer_type == 'bn':\n",
        "                if key not in self.v:\n",
        "                    self.v[key] = {'gamma': xp.zeros_like(layer.gamma), 'beta': xp.zeros_like(layer.beta)}\n",
        "\n",
        "                self.v[key]['gamma'] = self.momentum * self.v[key]['gamma'] - self.lr * layer.dgamma\n",
        "                self.v[key]['beta'] = self.momentum * self.v[key]['beta'] - self.lr * layer.dbeta\n",
        "\n",
        "                layer.gamma += self.v[key]['gamma']\n",
        "                layer.beta += self.v[key]['beta']\n",
        "\n",
        "            elif layer_type == 'fc':\n",
        "                if key not in self.v:\n",
        "                    self.v[key] = {'W': xp.zeros_like(layer.W), 'b': xp.zeros_like(layer.b)}\n",
        "\n",
        "                self.v[key]['W'] = self.momentum * self.v[key]['W'] - self.lr * layer.dW\n",
        "                self.v[key]['b'] = self.momentum * self.v[key]['b'] - self.lr * layer.db\n",
        "\n",
        "                layer.W += self.v[key]['W']\n",
        "                layer.b += self.v[key]['b']"
      ],
      "metadata": {
        "id": "DF5VZQni2FAb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Data Loader\n",
        "# ============================================\n",
        "\n",
        "class POCDataLoader:\n",
        "    def __init__(self, data_dir, img_size=224, batch_size=8):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.classes = ['Chorionic_villi', 'Decidual_tissue', 'Hemorrhage', 'Trophoblastic_tissue']\n",
        "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
        "\n",
        "    def load_data(self):\n",
        "        images = []\n",
        "        labels = []\n",
        "\n",
        "        print(f\"Loading data from {self.data_dir}...\")\n",
        "        for cls in self.classes:\n",
        "            cls_dir = os.path.join(self.data_dir, cls)\n",
        "            if not os.path.exists(cls_dir):\n",
        "                print(f\"Warning: {cls_dir} not found\")\n",
        "                continue\n",
        "\n",
        "            img_count = 0\n",
        "            for img_name in os.listdir(cls_dir):\n",
        "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img_path = os.path.join(cls_dir, img_name)\n",
        "                    try:\n",
        "                        img = Image.open(img_path).convert('RGB')\n",
        "                        img = img.resize((self.img_size, self.img_size))\n",
        "                        img_array = np.array(img).astype(np.float32) / 255.0\n",
        "\n",
        "                        mean = np.array([0.485, 0.456, 0.406])\n",
        "                        std = np.array([0.229, 0.224, 0.225])\n",
        "                        img_array = (img_array - mean) / std\n",
        "                        img_array = img_array.transpose(2, 0, 1)\n",
        "\n",
        "                        images.append(img_array)\n",
        "                        labels.append(self.class_to_idx[cls])\n",
        "                        img_count += 1\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading {img_path}: {e}\")\n",
        "\n",
        "            print(f\"  {cls}: {img_count} images\")\n",
        "\n",
        "        return np.array(images), np.array(labels)\n",
        "\n",
        "    def get_batches(self, images, labels, shuffle=True):\n",
        "        n = len(images)\n",
        "        indices = np.arange(n)\n",
        "        if shuffle:\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "        for start_idx in range(0, n, self.batch_size):\n",
        "            end_idx = min(start_idx + self.batch_size, n)\n",
        "            batch_indices = indices[start_idx:end_idx]\n",
        "            batch_x = to_gpu(images[batch_indices])\n",
        "            batch_y = to_gpu(labels[batch_indices])\n",
        "            yield batch_x, batch_y"
      ],
      "metadata": {
        "id": "9ktiodCI2H3Q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Train Func\n",
        "# ============================================\n",
        "\n",
        "def train_model(train_dir, test_dir, epochs=10, batch_size=4, lr=0.0001):\n",
        "    print(\"=\"*60)\n",
        "    print(\"ResNet-50 Training on POC Dataset\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Epochs: {epochs}, Batch: {batch_size}, LR: {lr}\")\n",
        "    print(f\"Device: {'GPU (CuPy)' if USE_GPU else 'CPU (NumPy)'}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    train_loader = POCDataLoader(train_dir, batch_size=batch_size)\n",
        "    test_loader = POCDataLoader(test_dir, batch_size=batch_size)\n",
        "\n",
        "    print(\"\\nLoading Data...\")\n",
        "    train_images, train_labels = train_loader.load_data()\n",
        "    print(f\"Training: {len(train_images)} samples\")\n",
        "\n",
        "    test_images, test_labels = test_loader.load_data()\n",
        "    print(f\"Testing: {len(test_images)} samples\")\n",
        "\n",
        "    if len(train_images) == 0 or len(test_images) == 0:\n",
        "        print(\"\\nNo data found! Check folder structure.\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"\\nClass Distribution:\")\n",
        "    for i, cls in enumerate(train_loader.classes):\n",
        "        train_count = np.sum(train_labels == i)\n",
        "        test_count = np.sum(test_labels == i)\n",
        "        print(f\"  {cls}: Train={train_count}, Test={test_count}\")\n",
        "\n",
        "    print(f\"\\n  Building Model...\")\n",
        "    model = ResNet50(num_classes=4)\n",
        "    criterion = CrossEntropyLoss()\n",
        "    optimizer = SGD(lr=lr, momentum=0.9)\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'test_acc': [], 'best_acc': 0}\n",
        "\n",
        "    print(f\"\\nTraining...\\n\" + \"=\"*60)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        print(\"-\"*60)\n",
        "\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        for batch_x, batch_y in train_loader.get_batches(train_images, train_labels):\n",
        "            output = model.forward(batch_x, train=True)\n",
        "            loss = criterion.forward(output, batch_y)\n",
        "\n",
        "            dout = criterion.backward()\n",
        "            model.backward(dout)\n",
        "            optimizer.update(model.get_params())\n",
        "\n",
        "            train_loss += loss\n",
        "            pred = xp.argmax(output, axis=1)\n",
        "            train_correct += int(xp.sum(pred == batch_y))\n",
        "            train_total += len(batch_y)\n",
        "            batch_count += 1\n",
        "\n",
        "            if batch_count % 10 == 0:\n",
        "                print(f\"  Batch {batch_count:3d}: Loss={loss:.4f}\")\n",
        "\n",
        "        train_loss /= batch_count\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        print(f\"\\n  Evaluating...\")\n",
        "        test_correct = 0\n",
        "        test_total = 0\n",
        "        for batch_x, batch_y in test_loader.get_batches(test_images, test_labels, shuffle=False):\n",
        "            output = model.forward(batch_x, train=False)\n",
        "            pred = xp.argmax(output, axis=1)\n",
        "            test_correct += int(xp.sum(pred == batch_y))\n",
        "            test_total += len(batch_y)\n",
        "\n",
        "        test_acc = test_correct / test_total\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['test_acc'].append(test_acc)\n",
        "\n",
        "        if test_acc > history['best_acc']:\n",
        "            history['best_acc'] = test_acc\n",
        "            print(f\"  New best!\")\n",
        "\n",
        "        print(f\"\\n  Results: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Test Acc={test_acc:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"Training Complete! Best Test Acc: {history['best_acc']:.4f}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"\\nSaving...\")\n",
        "    save_path = os.path.join(BASE_PATH, 'results')\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    with open(os.path.join(save_path, 'history.json'), 'w') as f:\n",
        "        json.dump(history, f, indent=2)\n",
        "\n",
        "    print(f\"Saved to {save_path}\")\n",
        "    plot_history(history, save_path)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def plot_history(history, save_path):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history['train_loss'], 'b-', linewidth=2)\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history['train_acc'], 'g-', label='Train', linewidth=2)\n",
        "    plt.plot(epochs, history['test_acc'], 'r-', label='Test', linewidth=2)\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_path, 'training_curve.png'), dpi=150)\n",
        "    plt.show()\n",
        "    print(\"Plot saved!\")"
      ],
      "metadata": {
        "id": "MnAH5vHA2K4V"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Run\n",
        "# ============================================\n",
        "\n",
        "print(\"Starting ResNet-50 Training\\n\")\n",
        "\n",
        "model, history = train_model(\n",
        "    train_dir=os.path.join(BASE_PATH, 'Training'),\n",
        "    test_dir=os.path.join(BASE_PATH, 'Testing'),\n",
        "    epochs=15,\n",
        "    batch_size=4,\n",
        "    lr=0.0001\n",
        ")\n",
        "\n",
        "print(\"\\nComplete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "91qvAMoR2MiS",
        "outputId": "d07ac125-452e-40b7-e23f-66facc4cfdaf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting ResNet-50 Training\n",
            "\n",
            "============================================================\n",
            "ResNet-50 Training on POC Dataset\n",
            "============================================================\n",
            "Epochs: 15, Batch: 4, LR: 0.0001\n",
            "Device: GPU (CuPy)\n",
            "============================================================\n",
            "\n",
            "Loading Data...\n",
            "Loading data from /content/drive/MyDrive/POC_Dataset/Training...\n",
            "  Chorionic_villi: 1391 images\n",
            "  Decidual_tissue: 926 images\n",
            "  Hemorrhage: 1138 images\n",
            "  Trophoblastic_tissue: 700 images\n",
            "Training: 4155 samples\n",
            "Loading data from /content/drive/MyDrive/POC_Dataset/Testing...\n",
            "  Chorionic_villi: 390 images\n",
            "  Decidual_tissue: 349 images\n",
            "  Hemorrhage: 421 images\n",
            "  Trophoblastic_tissue: 351 images\n",
            "Testing: 1511 samples\n",
            "\n",
            "Class Distribution:\n",
            "  Chorionic_villi: Train=1391, Test=390\n",
            "  Decidual_tissue: Train=926, Test=349\n",
            "  Hemorrhage: Train=1138, Test=421\n",
            "  Trophoblastic_tissue: Train=700, Test=351\n",
            "\n",
            "  Building Model...\n",
            "Initializing ResNet-50...\n",
            "ResNet-50 initialized\n",
            "\n",
            "Training...\n",
            "============================================================\n",
            "\n",
            "Epoch 1/15\n",
            "------------------------------------------------------------\n",
            "  Batch  10: Loss=1.5587\n",
            "  Batch  20: Loss=1.7148\n",
            "  Batch  30: Loss=1.2676\n",
            "  Batch  40: Loss=0.9967\n",
            "  Batch  50: Loss=1.9153\n",
            "  Batch  60: Loss=1.1021\n",
            "  Batch  70: Loss=1.2045\n",
            "  Batch  80: Loss=1.0001\n",
            "  Batch  90: Loss=1.7414\n",
            "  Batch 100: Loss=1.4429\n",
            "  Batch 110: Loss=1.5137\n",
            "  Batch 120: Loss=0.9371\n",
            "  Batch 130: Loss=1.2823\n",
            "  Batch 140: Loss=1.5424\n",
            "  Batch 150: Loss=1.0584\n",
            "  Batch 160: Loss=2.1639\n",
            "  Batch 170: Loss=1.4983\n",
            "  Batch 180: Loss=0.8572\n",
            "  Batch 190: Loss=1.3232\n",
            "  Batch 200: Loss=1.1622\n",
            "  Batch 210: Loss=1.6454\n",
            "  Batch 220: Loss=1.4694\n",
            "  Batch 230: Loss=0.9589\n",
            "  Batch 240: Loss=1.4021\n",
            "  Batch 250: Loss=1.3233\n",
            "  Batch 260: Loss=1.0623\n",
            "  Batch 270: Loss=0.9441\n",
            "  Batch 280: Loss=1.5577\n",
            "  Batch 290: Loss=1.3604\n",
            "  Batch 300: Loss=1.5384\n",
            "  Batch 310: Loss=1.4996\n",
            "  Batch 320: Loss=1.3383\n",
            "  Batch 330: Loss=1.2932\n",
            "  Batch 340: Loss=1.2907\n",
            "  Batch 350: Loss=0.9590\n",
            "  Batch 360: Loss=1.4667\n",
            "  Batch 370: Loss=1.4351\n",
            "  Batch 380: Loss=1.7753\n",
            "  Batch 390: Loss=0.9222\n",
            "  Batch 400: Loss=0.6636\n",
            "  Batch 410: Loss=1.4323\n",
            "  Batch 420: Loss=1.4607\n",
            "  Batch 430: Loss=0.8884\n",
            "  Batch 440: Loss=1.3947\n",
            "  Batch 450: Loss=1.1989\n",
            "  Batch 460: Loss=1.3389\n",
            "  Batch 470: Loss=1.2187\n",
            "  Batch 480: Loss=1.7910\n",
            "  Batch 490: Loss=1.1347\n",
            "  Batch 500: Loss=1.7543\n",
            "  Batch 510: Loss=1.0570\n",
            "  Batch 520: Loss=1.2027\n",
            "  Batch 530: Loss=1.2589\n",
            "  Batch 540: Loss=1.2790\n",
            "  Batch 550: Loss=1.4079\n",
            "  Batch 560: Loss=0.7291\n",
            "  Batch 570: Loss=0.9441\n",
            "  Batch 580: Loss=1.9217\n",
            "  Batch 590: Loss=0.8887\n",
            "  Batch 600: Loss=1.3374\n",
            "  Batch 610: Loss=0.9105\n",
            "  Batch 620: Loss=1.2456\n",
            "  Batch 630: Loss=0.9815\n",
            "  Batch 640: Loss=1.6914\n",
            "  Batch 650: Loss=1.1085\n",
            "  Batch 660: Loss=0.9733\n",
            "  Batch 670: Loss=1.0914\n",
            "  Batch 680: Loss=1.2874\n",
            "  Batch 690: Loss=1.1549\n",
            "  Batch 700: Loss=1.2069\n",
            "  Batch 710: Loss=1.6726\n",
            "  Batch 720: Loss=0.8821\n",
            "  Batch 730: Loss=1.9352\n",
            "  Batch 740: Loss=1.1391\n",
            "  Batch 750: Loss=1.6677\n",
            "  Batch 760: Loss=1.3179\n",
            "  Batch 770: Loss=0.9778\n",
            "  Batch 780: Loss=1.0300\n",
            "  Batch 790: Loss=0.5712\n",
            "  Batch 800: Loss=0.8072\n",
            "  Batch 810: Loss=3.2789\n",
            "  Batch 820: Loss=0.6589\n",
            "  Batch 830: Loss=0.7298\n",
            "  Batch 840: Loss=2.1639\n",
            "  Batch 850: Loss=0.9591\n",
            "  Batch 860: Loss=1.1068\n",
            "  Batch 870: Loss=0.7217\n",
            "  Batch 880: Loss=1.2424\n",
            "  Batch 890: Loss=1.3180\n",
            "  Batch 900: Loss=1.4921\n",
            "  Batch 910: Loss=0.8685\n",
            "  Batch 920: Loss=1.2658\n",
            "  Batch 930: Loss=1.3338\n",
            "  Batch 940: Loss=1.2858\n",
            "  Batch 950: Loss=1.2578\n",
            "  Batch 960: Loss=2.2193\n",
            "  Batch 970: Loss=1.4417\n",
            "  Batch 980: Loss=1.4483\n",
            "  Batch 990: Loss=0.6037\n",
            "  Batch 1000: Loss=1.1279\n",
            "  Batch 1010: Loss=0.6375\n",
            "  Batch 1020: Loss=0.6097\n",
            "  Batch 1030: Loss=0.6363\n",
            "\n",
            "  Evaluating...\n",
            "  New best!\n",
            "\n",
            "  Results: Train Loss=1.2511, Train Acc=0.4243, Test Acc=0.4977\n",
            "\n",
            "Epoch 2/15\n",
            "------------------------------------------------------------\n",
            "  Batch  10: Loss=2.4546\n",
            "  Batch  20: Loss=1.8063\n",
            "  Batch  30: Loss=0.8688\n",
            "  Batch  40: Loss=0.5614\n",
            "  Batch  50: Loss=1.4779\n",
            "  Batch  60: Loss=1.6257\n",
            "  Batch  70: Loss=0.5763\n",
            "  Batch  80: Loss=0.8203\n",
            "  Batch  90: Loss=1.1698\n",
            "  Batch 100: Loss=0.4604\n",
            "  Batch 110: Loss=1.5574\n",
            "  Batch 120: Loss=1.2377\n",
            "  Batch 130: Loss=1.8948\n",
            "  Batch 140: Loss=0.4038\n",
            "  Batch 150: Loss=0.9536\n",
            "  Batch 160: Loss=1.7025\n",
            "  Batch 170: Loss=0.6782\n",
            "  Batch 180: Loss=1.2051\n",
            "  Batch 190: Loss=1.2199\n",
            "  Batch 200: Loss=1.3780\n",
            "  Batch 210: Loss=1.5871\n",
            "  Batch 220: Loss=0.7190\n",
            "  Batch 230: Loss=1.2586\n",
            "  Batch 240: Loss=0.4733\n",
            "  Batch 250: Loss=0.8297\n",
            "  Batch 260: Loss=0.5912\n",
            "  Batch 270: Loss=0.8322\n",
            "  Batch 280: Loss=1.4027\n",
            "  Batch 290: Loss=0.8156\n",
            "  Batch 300: Loss=1.5174\n",
            "  Batch 310: Loss=0.8156\n",
            "  Batch 320: Loss=1.0241\n",
            "  Batch 330: Loss=0.7347\n",
            "  Batch 340: Loss=1.3143\n",
            "  Batch 350: Loss=1.1399\n",
            "  Batch 360: Loss=1.2764\n",
            "  Batch 370: Loss=0.8598\n",
            "  Batch 380: Loss=2.3004\n",
            "  Batch 390: Loss=0.5797\n",
            "  Batch 400: Loss=0.8926\n",
            "  Batch 410: Loss=0.9646\n",
            "  Batch 420: Loss=1.4070\n",
            "  Batch 430: Loss=1.0761\n",
            "  Batch 440: Loss=0.9283\n",
            "  Batch 450: Loss=2.0357\n",
            "  Batch 460: Loss=0.9352\n",
            "  Batch 470: Loss=1.3942\n",
            "  Batch 480: Loss=0.8309\n",
            "  Batch 490: Loss=0.7477\n",
            "  Batch 500: Loss=0.7193\n",
            "  Batch 510: Loss=0.6515\n",
            "  Batch 520: Loss=1.0673\n",
            "  Batch 530: Loss=0.3339\n",
            "  Batch 540: Loss=1.0895\n",
            "  Batch 550: Loss=1.3287\n",
            "  Batch 560: Loss=0.5792\n",
            "  Batch 570: Loss=0.8936\n",
            "  Batch 580: Loss=0.7813\n",
            "  Batch 590: Loss=0.4065\n",
            "  Batch 600: Loss=1.4879\n",
            "  Batch 610: Loss=1.5386\n",
            "  Batch 620: Loss=1.0054\n",
            "  Batch 630: Loss=2.2085\n",
            "  Batch 640: Loss=1.4682\n",
            "  Batch 650: Loss=0.7707\n",
            "  Batch 660: Loss=0.9985\n",
            "  Batch 670: Loss=1.1942\n",
            "  Batch 680: Loss=1.1360\n",
            "  Batch 690: Loss=0.4472\n",
            "  Batch 700: Loss=1.6938\n",
            "  Batch 710: Loss=0.9669\n",
            "  Batch 720: Loss=0.6917\n",
            "  Batch 730: Loss=0.9777\n",
            "  Batch 740: Loss=0.6041\n",
            "  Batch 750: Loss=0.9594\n",
            "  Batch 760: Loss=1.0869\n",
            "  Batch 770: Loss=2.0673\n",
            "  Batch 780: Loss=0.4477\n",
            "  Batch 790: Loss=1.3326\n",
            "  Batch 800: Loss=0.7643\n",
            "  Batch 810: Loss=0.5772\n",
            "  Batch 820: Loss=1.7757\n",
            "  Batch 830: Loss=0.5638\n",
            "  Batch 840: Loss=0.9080\n",
            "  Batch 850: Loss=1.0073\n",
            "  Batch 860: Loss=1.1314\n",
            "  Batch 870: Loss=0.6404\n",
            "  Batch 880: Loss=0.9333\n",
            "  Batch 890: Loss=1.6425\n",
            "  Batch 900: Loss=1.3474\n",
            "  Batch 910: Loss=0.5300\n",
            "  Batch 920: Loss=1.2246\n",
            "  Batch 930: Loss=0.5443\n",
            "  Batch 940: Loss=1.1580\n",
            "  Batch 950: Loss=1.2284\n",
            "  Batch 960: Loss=1.1191\n",
            "  Batch 970: Loss=1.1686\n",
            "  Batch 980: Loss=0.5065\n",
            "  Batch 990: Loss=0.8414\n",
            "  Batch 1000: Loss=0.5874\n",
            "  Batch 1010: Loss=1.5795\n",
            "  Batch 1020: Loss=1.5571\n",
            "  Batch 1030: Loss=0.7553\n",
            "\n",
            "  Evaluating...\n",
            "\n",
            "  Results: Train Loss=1.0758, Train Acc=0.5343, Test Acc=0.4858\n",
            "\n",
            "Epoch 3/15\n",
            "------------------------------------------------------------\n",
            "  Batch  10: Loss=0.5153\n",
            "  Batch  20: Loss=0.8804\n",
            "  Batch  30: Loss=1.2656\n",
            "  Batch  40: Loss=0.5974\n",
            "  Batch  50: Loss=1.2106\n",
            "  Batch  60: Loss=0.4820\n",
            "  Batch  70: Loss=1.3437\n",
            "  Batch  80: Loss=1.6777\n",
            "  Batch  90: Loss=1.2979\n",
            "  Batch 100: Loss=0.9457\n",
            "  Batch 110: Loss=1.3253\n",
            "  Batch 120: Loss=0.9037\n",
            "  Batch 130: Loss=1.8263\n",
            "  Batch 140: Loss=1.0124\n",
            "  Batch 150: Loss=0.3528\n",
            "  Batch 160: Loss=1.5491\n",
            "  Batch 170: Loss=1.1242\n",
            "  Batch 180: Loss=0.7650\n",
            "  Batch 190: Loss=1.2395\n",
            "  Batch 200: Loss=0.6253\n",
            "  Batch 210: Loss=1.0501\n",
            "  Batch 220: Loss=1.2773\n",
            "  Batch 230: Loss=0.5439\n",
            "  Batch 240: Loss=0.9062\n",
            "  Batch 250: Loss=1.3765\n",
            "  Batch 260: Loss=0.4652\n",
            "  Batch 270: Loss=0.8572\n",
            "  Batch 280: Loss=1.8569\n",
            "  Batch 290: Loss=0.5750\n",
            "  Batch 300: Loss=0.7825\n",
            "  Batch 310: Loss=1.0474\n",
            "  Batch 320: Loss=0.3960\n",
            "  Batch 330: Loss=1.6509\n",
            "  Batch 340: Loss=0.3257\n",
            "  Batch 350: Loss=1.3109\n",
            "  Batch 360: Loss=0.6948\n",
            "  Batch 370: Loss=0.5931\n",
            "  Batch 380: Loss=0.6566\n",
            "  Batch 390: Loss=1.2078\n",
            "  Batch 400: Loss=0.4403\n",
            "  Batch 410: Loss=1.1310\n",
            "  Batch 420: Loss=1.8299\n",
            "  Batch 430: Loss=1.6984\n",
            "  Batch 440: Loss=1.4010\n",
            "  Batch 450: Loss=0.9306\n",
            "  Batch 460: Loss=0.7344\n",
            "  Batch 470: Loss=0.5343\n",
            "  Batch 480: Loss=1.1085\n",
            "  Batch 490: Loss=0.8312\n",
            "  Batch 500: Loss=0.8339\n",
            "  Batch 510: Loss=1.1188\n",
            "  Batch 520: Loss=0.7475\n",
            "  Batch 530: Loss=1.5873\n",
            "  Batch 540: Loss=0.8330\n",
            "  Batch 550: Loss=0.6631\n",
            "  Batch 560: Loss=0.4948\n",
            "  Batch 570: Loss=0.9561\n",
            "  Batch 580: Loss=0.6790\n",
            "  Batch 590: Loss=1.4179\n",
            "  Batch 600: Loss=0.9746\n",
            "  Batch 610: Loss=0.5010\n",
            "  Batch 620: Loss=1.1696\n",
            "  Batch 630: Loss=0.6432\n",
            "  Batch 640: Loss=2.5295\n",
            "  Batch 650: Loss=1.0188\n",
            "  Batch 660: Loss=1.2703\n",
            "  Batch 670: Loss=0.9839\n",
            "  Batch 680: Loss=0.5873\n",
            "  Batch 690: Loss=1.1080\n",
            "  Batch 700: Loss=1.7144\n",
            "  Batch 710: Loss=1.1502\n",
            "  Batch 720: Loss=0.7789\n",
            "  Batch 730: Loss=0.3336\n",
            "  Batch 740: Loss=1.5070\n",
            "  Batch 750: Loss=1.2482\n",
            "  Batch 760: Loss=1.1889\n",
            "  Batch 770: Loss=0.6401\n",
            "  Batch 780: Loss=0.5017\n",
            "  Batch 790: Loss=0.8166\n",
            "  Batch 800: Loss=1.1225\n",
            "  Batch 810: Loss=0.4468\n",
            "  Batch 820: Loss=0.6209\n",
            "  Batch 830: Loss=0.7994\n",
            "  Batch 840: Loss=1.3326\n",
            "  Batch 850: Loss=1.5753\n",
            "  Batch 860: Loss=0.7344\n",
            "  Batch 870: Loss=0.7940\n",
            "  Batch 880: Loss=1.5957\n",
            "  Batch 890: Loss=1.3251\n",
            "  Batch 900: Loss=1.0786\n",
            "  Batch 910: Loss=0.8910\n",
            "  Batch 920: Loss=0.6689\n",
            "  Batch 930: Loss=1.0286\n",
            "  Batch 940: Loss=0.9940\n",
            "  Batch 950: Loss=0.9240\n",
            "  Batch 960: Loss=0.8650\n",
            "  Batch 970: Loss=1.0778\n",
            "  Batch 980: Loss=2.0319\n",
            "  Batch 990: Loss=0.6788\n",
            "  Batch 1000: Loss=0.9124\n",
            "  Batch 1010: Loss=0.4924\n",
            "  Batch 1020: Loss=0.7140\n",
            "  Batch 1030: Loss=0.9104\n",
            "\n",
            "  Evaluating...\n",
            "  New best!\n",
            "\n",
            "  Results: Train Loss=1.0271, Train Acc=0.5651, Test Acc=0.5963\n",
            "\n",
            "Epoch 4/15\n",
            "------------------------------------------------------------\n",
            "  Batch  10: Loss=0.5892\n",
            "  Batch  20: Loss=0.4482\n",
            "  Batch  30: Loss=1.4998\n",
            "  Batch  40: Loss=1.5289\n",
            "  Batch  50: Loss=2.2173\n",
            "  Batch  60: Loss=0.4484\n",
            "  Batch  70: Loss=0.8233\n",
            "  Batch  80: Loss=1.8548\n",
            "  Batch  90: Loss=0.9372\n",
            "  Batch 100: Loss=0.9409\n",
            "  Batch 110: Loss=1.6010\n",
            "  Batch 120: Loss=0.8242\n",
            "  Batch 130: Loss=0.8593\n",
            "  Batch 140: Loss=1.3049\n",
            "  Batch 150: Loss=1.0660\n",
            "  Batch 160: Loss=0.4000\n",
            "  Batch 170: Loss=1.4989\n",
            "  Batch 180: Loss=1.6369\n",
            "  Batch 190: Loss=1.4976\n",
            "  Batch 200: Loss=0.9124\n",
            "  Batch 210: Loss=0.8964\n",
            "  Batch 220: Loss=1.0253\n",
            "  Batch 230: Loss=0.2906\n",
            "  Batch 240: Loss=0.8428\n",
            "  Batch 250: Loss=1.0670\n",
            "  Batch 260: Loss=2.4464\n",
            "  Batch 270: Loss=0.9379\n",
            "  Batch 280: Loss=2.0310\n",
            "  Batch 290: Loss=0.7816\n",
            "  Batch 300: Loss=0.4995\n",
            "  Batch 310: Loss=0.9535\n",
            "  Batch 320: Loss=0.9054\n",
            "  Batch 330: Loss=0.6119\n",
            "  Batch 340: Loss=0.8313\n",
            "  Batch 350: Loss=0.9318\n",
            "  Batch 360: Loss=0.8004\n",
            "  Batch 370: Loss=1.5184\n",
            "  Batch 380: Loss=0.6714\n",
            "  Batch 390: Loss=0.9352\n",
            "  Batch 400: Loss=1.0613\n",
            "  Batch 410: Loss=0.9506\n",
            "  Batch 420: Loss=0.4568\n",
            "  Batch 430: Loss=1.7859\n",
            "  Batch 440: Loss=0.6057\n",
            "  Batch 450: Loss=1.2713\n",
            "  Batch 460: Loss=2.2597\n",
            "  Batch 470: Loss=2.7656\n",
            "  Batch 480: Loss=0.4009\n",
            "  Batch 490: Loss=0.7304\n",
            "  Batch 500: Loss=1.9116\n",
            "  Batch 510: Loss=0.4451\n",
            "  Batch 520: Loss=0.9659\n",
            "  Batch 530: Loss=1.0204\n",
            "  Batch 540: Loss=0.9250\n",
            "  Batch 550: Loss=0.8453\n",
            "  Batch 560: Loss=0.6052\n",
            "  Batch 570: Loss=1.0445\n",
            "  Batch 580: Loss=1.4515\n",
            "  Batch 590: Loss=0.4566\n",
            "  Batch 600: Loss=0.9927\n",
            "  Batch 610: Loss=0.9813\n",
            "  Batch 620: Loss=0.9459\n",
            "  Batch 630: Loss=1.4679\n",
            "  Batch 640: Loss=1.4872\n",
            "  Batch 650: Loss=1.3408\n",
            "  Batch 660: Loss=1.6680\n",
            "  Batch 670: Loss=0.4816\n",
            "  Batch 680: Loss=0.8283\n",
            "  Batch 690: Loss=0.3193\n",
            "  Batch 700: Loss=0.4052\n",
            "  Batch 710: Loss=1.3001\n",
            "  Batch 720: Loss=1.1127\n",
            "  Batch 730: Loss=0.9373\n",
            "  Batch 740: Loss=0.4325\n",
            "  Batch 750: Loss=0.4293\n",
            "  Batch 760: Loss=1.9884\n",
            "  Batch 770: Loss=0.5482\n",
            "  Batch 780: Loss=1.1943\n",
            "  Batch 790: Loss=1.0884\n",
            "  Batch 800: Loss=0.5370\n",
            "  Batch 810: Loss=0.4490\n",
            "  Batch 820: Loss=0.3941\n",
            "  Batch 830: Loss=1.0790\n",
            "  Batch 840: Loss=0.6088\n",
            "  Batch 850: Loss=0.8688\n",
            "  Batch 860: Loss=0.3208\n",
            "  Batch 870: Loss=0.2061\n",
            "  Batch 880: Loss=1.0430\n",
            "  Batch 890: Loss=0.4040\n",
            "  Batch 900: Loss=0.8924\n",
            "  Batch 910: Loss=0.3916\n",
            "  Batch 920: Loss=1.6129\n",
            "  Batch 930: Loss=0.9306\n",
            "  Batch 940: Loss=0.7325\n",
            "  Batch 950: Loss=1.2595\n",
            "  Batch 960: Loss=2.3930\n",
            "  Batch 970: Loss=1.2311\n",
            "  Batch 980: Loss=0.4769\n",
            "  Batch 990: Loss=0.9049\n",
            "  Batch 1000: Loss=0.6642\n",
            "  Batch 1010: Loss=2.0720\n",
            "  Batch 1020: Loss=1.3078\n",
            "  Batch 1030: Loss=2.5439\n",
            "\n",
            "  Evaluating...\n",
            "\n",
            "  Results: Train Loss=0.9864, Train Acc=0.5933, Test Acc=0.4573\n",
            "\n",
            "Epoch 5/15\n",
            "------------------------------------------------------------\n",
            "  Batch  10: Loss=0.9543\n",
            "  Batch  20: Loss=0.2963\n",
            "  Batch  30: Loss=0.2802\n",
            "  Batch  40: Loss=0.4917\n",
            "  Batch  50: Loss=0.4506\n",
            "  Batch  60: Loss=0.6258\n",
            "  Batch  70: Loss=0.8415\n",
            "  Batch  80: Loss=1.1007\n",
            "  Batch  90: Loss=0.4239\n",
            "  Batch 100: Loss=1.3199\n",
            "  Batch 110: Loss=0.2804\n",
            "  Batch 120: Loss=2.5253\n",
            "  Batch 130: Loss=0.8184\n",
            "  Batch 140: Loss=0.7424\n",
            "  Batch 150: Loss=0.6011\n",
            "  Batch 160: Loss=0.6744\n",
            "  Batch 170: Loss=1.2411\n",
            "  Batch 180: Loss=0.8720\n",
            "  Batch 190: Loss=0.7971\n",
            "  Batch 200: Loss=1.0658\n",
            "  Batch 210: Loss=0.4825\n",
            "  Batch 220: Loss=0.9479\n",
            "  Batch 230: Loss=1.0303\n",
            "  Batch 240: Loss=2.5328\n",
            "  Batch 250: Loss=1.1368\n",
            "  Batch 260: Loss=0.4756\n",
            "  Batch 270: Loss=1.4921\n",
            "  Batch 280: Loss=2.4947\n",
            "  Batch 290: Loss=1.2723\n",
            "  Batch 300: Loss=1.0166\n",
            "  Batch 310: Loss=0.7439\n",
            "  Batch 320: Loss=0.5502\n",
            "  Batch 330: Loss=1.2036\n",
            "  Batch 340: Loss=0.4463\n",
            "  Batch 350: Loss=0.4587\n",
            "  Batch 360: Loss=0.6828\n",
            "  Batch 370: Loss=0.8562\n",
            "  Batch 380: Loss=0.5538\n",
            "  Batch 390: Loss=1.4572\n",
            "  Batch 400: Loss=0.4072\n",
            "  Batch 410: Loss=1.0203\n",
            "  Batch 420: Loss=1.5416\n",
            "  Batch 430: Loss=0.1242\n",
            "  Batch 440: Loss=0.6475\n",
            "  Batch 450: Loss=1.6191\n",
            "  Batch 460: Loss=0.9549\n",
            "  Batch 470: Loss=1.0871\n",
            "  Batch 480: Loss=1.4070\n",
            "  Batch 490: Loss=0.4423\n",
            "  Batch 500: Loss=1.2152\n",
            "  Batch 510: Loss=0.8697\n",
            "  Batch 520: Loss=1.5162\n",
            "  Batch 530: Loss=1.0667\n",
            "  Batch 540: Loss=0.7337\n",
            "  Batch 550: Loss=2.3461\n",
            "  Batch 560: Loss=0.8571\n",
            "  Batch 570: Loss=2.2315\n",
            "  Batch 580: Loss=0.8320\n",
            "  Batch 590: Loss=1.2292\n",
            "  Batch 600: Loss=0.5775\n",
            "  Batch 610: Loss=0.3390\n",
            "  Batch 620: Loss=0.9188\n",
            "  Batch 630: Loss=0.3175\n",
            "  Batch 640: Loss=1.1461\n",
            "  Batch 650: Loss=1.0905\n",
            "  Batch 660: Loss=0.6884\n",
            "  Batch 670: Loss=0.2897\n",
            "  Batch 680: Loss=0.8045\n",
            "  Batch 690: Loss=0.5068\n",
            "  Batch 700: Loss=2.3894\n",
            "  Batch 710: Loss=1.8448\n",
            "  Batch 720: Loss=0.9906\n",
            "  Batch 730: Loss=0.9140\n",
            "  Batch 740: Loss=1.2125\n",
            "  Batch 750: Loss=1.2573\n",
            "  Batch 760: Loss=0.3335\n",
            "  Batch 770: Loss=1.7102\n",
            "  Batch 780: Loss=1.2807\n",
            "  Batch 790: Loss=0.6492\n",
            "  Batch 800: Loss=0.7611\n",
            "  Batch 810: Loss=1.2642\n",
            "  Batch 820: Loss=1.9887\n",
            "  Batch 830: Loss=0.4777\n",
            "  Batch 840: Loss=0.8090\n",
            "  Batch 850: Loss=0.5937\n",
            "  Batch 860: Loss=0.4067\n",
            "  Batch 870: Loss=0.5306\n",
            "  Batch 880: Loss=2.3192\n",
            "  Batch 890: Loss=2.3555\n",
            "  Batch 900: Loss=0.8093\n",
            "  Batch 910: Loss=1.1687\n",
            "  Batch 920: Loss=1.6082\n",
            "  Batch 930: Loss=0.8622\n",
            "  Batch 940: Loss=0.6401\n",
            "  Batch 950: Loss=0.4161\n",
            "  Batch 960: Loss=0.6871\n",
            "  Batch 970: Loss=2.0077\n",
            "  Batch 980: Loss=0.7510\n",
            "  Batch 990: Loss=0.7123\n",
            "  Batch 1000: Loss=0.8007\n",
            "  Batch 1010: Loss=1.7391\n",
            "  Batch 1020: Loss=0.9502\n",
            "  Batch 1030: Loss=1.3585\n",
            "\n",
            "  Evaluating...\n",
            "  New best!\n",
            "\n",
            "  Results: Train Loss=0.9443, Train Acc=0.6171, Test Acc=0.6294\n",
            "\n",
            "Epoch 6/15\n",
            "------------------------------------------------------------\n",
            "  Batch  10: Loss=1.2366\n",
            "  Batch  20: Loss=0.6232\n",
            "  Batch  30: Loss=0.8279\n",
            "  Batch  40: Loss=0.5572\n",
            "  Batch  50: Loss=0.8096\n",
            "  Batch  60: Loss=0.4972\n",
            "  Batch  70: Loss=0.3871\n",
            "  Batch  80: Loss=1.5401\n",
            "  Batch  90: Loss=1.3523\n",
            "  Batch 100: Loss=0.3948\n",
            "  Batch 110: Loss=0.7100\n",
            "  Batch 120: Loss=1.8049\n",
            "  Batch 130: Loss=0.5252\n",
            "  Batch 140: Loss=1.5373\n",
            "  Batch 150: Loss=0.9879\n",
            "  Batch 160: Loss=1.3708\n",
            "  Batch 170: Loss=0.3387\n",
            "  Batch 180: Loss=2.2604\n",
            "  Batch 190: Loss=0.2966\n",
            "  Batch 200: Loss=1.3072\n",
            "  Batch 210: Loss=3.0419\n",
            "  Batch 220: Loss=1.3089\n",
            "  Batch 230: Loss=1.0365\n",
            "  Batch 240: Loss=1.1393\n",
            "  Batch 250: Loss=0.4325\n",
            "  Batch 260: Loss=0.8090\n",
            "  Batch 270: Loss=0.6568\n",
            "  Batch 280: Loss=1.5727\n",
            "  Batch 290: Loss=1.0084\n",
            "  Batch 300: Loss=0.8589\n",
            "  Batch 310: Loss=0.4706\n",
            "  Batch 320: Loss=0.8909\n",
            "  Batch 330: Loss=0.5464\n",
            "  Batch 340: Loss=0.8720\n",
            "  Batch 350: Loss=1.4557\n",
            "  Batch 360: Loss=0.8412\n",
            "  Batch 370: Loss=0.4923\n",
            "  Batch 380: Loss=0.2057\n",
            "  Batch 390: Loss=1.1731\n",
            "  Batch 400: Loss=1.1839\n",
            "  Batch 410: Loss=0.8236\n",
            "  Batch 420: Loss=1.0554\n",
            "  Batch 430: Loss=0.9919\n",
            "  Batch 440: Loss=1.1462\n",
            "  Batch 450: Loss=0.6230\n",
            "  Batch 460: Loss=0.6221\n",
            "  Batch 470: Loss=1.0477\n",
            "  Batch 480: Loss=0.6625\n",
            "  Batch 490: Loss=1.1870\n",
            "  Batch 500: Loss=0.6652\n",
            "  Batch 510: Loss=1.8474\n",
            "  Batch 520: Loss=0.2977\n",
            "  Batch 530: Loss=0.2528\n",
            "  Batch 540: Loss=0.6238\n",
            "  Batch 550: Loss=0.9692\n",
            "  Batch 560: Loss=1.7837\n",
            "  Batch 570: Loss=1.9515\n",
            "  Batch 580: Loss=0.3706\n",
            "  Batch 590: Loss=0.6831\n",
            "  Batch 600: Loss=0.4079\n",
            "  Batch 610: Loss=2.7161\n",
            "  Batch 620: Loss=1.5499\n",
            "  Batch 630: Loss=1.8658\n",
            "  Batch 640: Loss=1.6069\n",
            "  Batch 650: Loss=0.4211\n",
            "  Batch 660: Loss=0.5918\n",
            "  Batch 670: Loss=0.8690\n",
            "  Batch 680: Loss=0.7933\n",
            "  Batch 690: Loss=0.4130\n",
            "  Batch 700: Loss=0.9679\n",
            "  Batch 710: Loss=0.8850\n",
            "  Batch 720: Loss=0.9101\n",
            "  Batch 730: Loss=0.9029\n",
            "  Batch 740: Loss=1.4765\n",
            "  Batch 750: Loss=0.5322\n",
            "  Batch 760: Loss=0.5906\n",
            "  Batch 770: Loss=0.2391\n",
            "  Batch 780: Loss=1.1516\n",
            "  Batch 790: Loss=0.1959\n",
            "  Batch 800: Loss=1.0635\n",
            "  Batch 810: Loss=0.8274\n",
            "  Batch 820: Loss=1.5460\n",
            "  Batch 830: Loss=1.6949\n",
            "  Batch 840: Loss=1.6636\n",
            "  Batch 850: Loss=1.4663\n",
            "  Batch 860: Loss=0.7628\n",
            "  Batch 870: Loss=1.4376\n",
            "  Batch 880: Loss=0.8864\n",
            "  Batch 890: Loss=0.3776\n",
            "  Batch 900: Loss=0.3557\n",
            "  Batch 910: Loss=1.4075\n",
            "  Batch 920: Loss=1.4479\n",
            "  Batch 930: Loss=1.1075\n",
            "  Batch 940: Loss=1.9371\n",
            "  Batch 950: Loss=0.5411\n",
            "  Batch 960: Loss=0.6675\n",
            "  Batch 970: Loss=0.4118\n",
            "  Batch 980: Loss=0.3979\n",
            "  Batch 990: Loss=0.9165\n",
            "  Batch 1000: Loss=1.2895\n",
            "  Batch 1010: Loss=0.3081\n",
            "  Batch 1020: Loss=0.8030\n",
            "  Batch 1030: Loss=0.8537\n",
            "\n",
            "  Evaluating...\n",
            "  New best!\n",
            "\n",
            "  Results: Train Loss=0.9320, Train Acc=0.6231, Test Acc=0.6499\n",
            "\n",
            "Epoch 7/15\n",
            "------------------------------------------------------------\n",
            "  Batch  10: Loss=0.7299\n",
            "  Batch  20: Loss=1.6400\n",
            "  Batch  30: Loss=1.5365\n",
            "  Batch  40: Loss=2.0474\n",
            "  Batch  50: Loss=0.4864\n",
            "  Batch  60: Loss=1.7979\n",
            "  Batch  70: Loss=0.5482\n",
            "  Batch  80: Loss=1.0381\n",
            "  Batch  90: Loss=0.5649\n",
            "  Batch 100: Loss=0.6651\n",
            "  Batch 110: Loss=0.4702\n",
            "  Batch 120: Loss=0.1914\n",
            "  Batch 130: Loss=0.8375\n",
            "  Batch 140: Loss=0.6877\n",
            "  Batch 150: Loss=1.4681\n",
            "  Batch 160: Loss=0.6195\n",
            "  Batch 170: Loss=0.5223\n",
            "  Batch 180: Loss=1.1298\n",
            "  Batch 190: Loss=0.4225\n",
            "  Batch 200: Loss=1.1043\n",
            "  Batch 210: Loss=0.4847\n",
            "  Batch 220: Loss=0.9308\n",
            "  Batch 230: Loss=1.7145\n",
            "  Batch 240: Loss=0.2116\n",
            "  Batch 250: Loss=0.5276\n",
            "  Batch 260: Loss=0.5668\n",
            "  Batch 270: Loss=0.4695\n",
            "  Batch 280: Loss=0.3222\n",
            "  Batch 290: Loss=1.0260\n",
            "  Batch 300: Loss=0.2336\n",
            "  Batch 310: Loss=0.9969\n",
            "  Batch 320: Loss=1.3908\n",
            "  Batch 330: Loss=1.7233\n",
            "  Batch 340: Loss=0.7236\n",
            "  Batch 350: Loss=0.8784\n",
            "  Batch 360: Loss=0.3468\n",
            "  Batch 370: Loss=1.6254\n",
            "  Batch 380: Loss=1.1490\n",
            "  Batch 390: Loss=0.6790\n",
            "  Batch 400: Loss=1.1471\n",
            "  Batch 410: Loss=1.3675\n",
            "  Batch 420: Loss=1.2162\n",
            "  Batch 430: Loss=2.2517\n",
            "  Batch 440: Loss=0.4678\n",
            "  Batch 450: Loss=0.2106\n",
            "  Batch 460: Loss=1.0236\n",
            "  Batch 470: Loss=0.2787\n",
            "  Batch 480: Loss=1.2169\n",
            "  Batch 490: Loss=1.1652\n",
            "  Batch 500: Loss=0.4994\n",
            "  Batch 510: Loss=1.3827\n",
            "  Batch 520: Loss=1.2710\n",
            "  Batch 530: Loss=1.1459\n",
            "  Batch 540: Loss=0.3399\n",
            "  Batch 550: Loss=1.1926\n",
            "  Batch 560: Loss=2.0648\n",
            "  Batch 570: Loss=0.6712\n",
            "  Batch 580: Loss=0.5919\n",
            "  Batch 590: Loss=0.9296\n",
            "  Batch 600: Loss=1.0038\n",
            "  Batch 610: Loss=0.9719\n",
            "  Batch 620: Loss=1.1337\n",
            "  Batch 630: Loss=0.9477\n",
            "  Batch 640: Loss=0.7995\n",
            "  Batch 650: Loss=0.7639\n",
            "  Batch 660: Loss=1.3812\n",
            "  Batch 670: Loss=0.3710\n",
            "  Batch 680: Loss=0.3897\n",
            "  Batch 690: Loss=0.6699\n",
            "  Batch 700: Loss=1.7385\n",
            "  Batch 710: Loss=0.4783\n",
            "  Batch 720: Loss=0.7386\n",
            "  Batch 730: Loss=1.0870\n",
            "  Batch 740: Loss=0.3842\n",
            "  Batch 750: Loss=0.1081\n",
            "  Batch 760: Loss=1.3390\n",
            "  Batch 770: Loss=2.0245\n",
            "  Batch 780: Loss=0.9022\n",
            "  Batch 790: Loss=0.2832\n",
            "  Batch 800: Loss=0.4330\n",
            "  Batch 810: Loss=0.8845\n",
            "  Batch 820: Loss=1.9332\n",
            "  Batch 830: Loss=0.7960\n",
            "  Batch 840: Loss=0.9018\n",
            "  Batch 850: Loss=0.6132\n",
            "  Batch 860: Loss=1.2856\n",
            "  Batch 870: Loss=0.3133\n",
            "  Batch 880: Loss=2.0418\n",
            "  Batch 890: Loss=0.4336\n",
            "  Batch 900: Loss=0.4249\n",
            "  Batch 910: Loss=0.5118\n",
            "  Batch 920: Loss=0.4356\n",
            "  Batch 930: Loss=0.3557\n",
            "  Batch 940: Loss=0.9138\n",
            "  Batch 950: Loss=2.1519\n",
            "  Batch 960: Loss=1.2364\n",
            "  Batch 970: Loss=0.4222\n",
            "  Batch 980: Loss=0.3019\n",
            "  Batch 990: Loss=1.2381\n",
            "  Batch 1000: Loss=0.3500\n",
            "  Batch 1010: Loss=0.8369\n",
            "  Batch 1020: Loss=0.2841\n",
            "  Batch 1030: Loss=0.7200\n",
            "\n",
            "  Evaluating...\n",
            "\n",
            "  Results: Train Loss=0.8904, Train Acc=0.6445, Test Acc=0.5864\n",
            "\n",
            "Epoch 8/15\n",
            "------------------------------------------------------------\n",
            "  Batch  10: Loss=0.4347\n",
            "  Batch  20: Loss=0.2085\n",
            "  Batch  30: Loss=1.2637\n",
            "  Batch  40: Loss=0.4813\n",
            "  Batch  50: Loss=0.3171\n",
            "  Batch  60: Loss=1.1654\n",
            "  Batch  70: Loss=1.1060\n",
            "  Batch  80: Loss=0.3880\n",
            "  Batch  90: Loss=1.0053\n",
            "  Batch 100: Loss=1.5822\n",
            "  Batch 110: Loss=0.5083\n",
            "  Batch 120: Loss=1.0833\n",
            "  Batch 130: Loss=1.0375\n",
            "  Batch 140: Loss=1.3519\n",
            "  Batch 150: Loss=0.4672\n",
            "  Batch 160: Loss=1.1134\n",
            "  Batch 170: Loss=0.1565\n",
            "  Batch 180: Loss=0.7971\n",
            "  Batch 190: Loss=1.7384\n",
            "  Batch 200: Loss=1.1590\n",
            "  Batch 210: Loss=0.8777\n",
            "  Batch 220: Loss=0.8999\n",
            "  Batch 230: Loss=1.3771\n",
            "  Batch 240: Loss=1.8208\n",
            "  Batch 250: Loss=0.2663\n",
            "  Batch 260: Loss=0.4445\n",
            "  Batch 270: Loss=0.5710\n",
            "  Batch 280: Loss=0.2486\n",
            "  Batch 290: Loss=0.7599\n",
            "  Batch 300: Loss=0.8807\n",
            "  Batch 310: Loss=0.5118\n",
            "  Batch 320: Loss=1.1056\n",
            "  Batch 330: Loss=1.2948\n",
            "  Batch 340: Loss=1.5243\n",
            "  Batch 350: Loss=1.5135\n",
            "  Batch 360: Loss=0.4082\n",
            "  Batch 370: Loss=0.1360\n",
            "  Batch 380: Loss=0.6821\n",
            "  Batch 390: Loss=0.8157\n",
            "  Batch 400: Loss=1.5734\n",
            "  Batch 410: Loss=1.8186\n",
            "  Batch 420: Loss=0.3028\n",
            "  Batch 430: Loss=1.2835\n",
            "  Batch 440: Loss=1.0283\n",
            "  Batch 450: Loss=0.2361\n",
            "  Batch 460: Loss=1.1315\n",
            "  Batch 470: Loss=0.4975\n",
            "  Batch 480: Loss=0.5363\n",
            "  Batch 490: Loss=1.2869\n",
            "  Batch 500: Loss=0.4451\n",
            "  Batch 510: Loss=1.0241\n",
            "  Batch 520: Loss=0.7112\n",
            "  Batch 530: Loss=0.8958\n",
            "  Batch 540: Loss=0.7279\n",
            "  Batch 550: Loss=0.4484\n",
            "  Batch 560: Loss=0.1911\n",
            "  Batch 570: Loss=0.3179\n",
            "  Batch 580: Loss=0.4730\n",
            "  Batch 590: Loss=1.0165\n",
            "  Batch 600: Loss=1.0225\n",
            "  Batch 610: Loss=0.4510\n",
            "  Batch 620: Loss=0.6740\n",
            "  Batch 630: Loss=0.8598\n",
            "  Batch 640: Loss=1.0677\n",
            "  Batch 650: Loss=0.5054\n",
            "  Batch 660: Loss=1.5643\n",
            "  Batch 670: Loss=0.2469\n",
            "  Batch 680: Loss=0.2036\n",
            "  Batch 690: Loss=0.1632\n",
            "  Batch 700: Loss=0.4402\n",
            "  Batch 710: Loss=0.2820\n",
            "  Batch 720: Loss=1.1611\n",
            "  Batch 730: Loss=1.7290\n",
            "  Batch 740: Loss=0.4289\n",
            "  Batch 750: Loss=0.9758\n",
            "  Batch 760: Loss=0.7065\n",
            "  Batch 770: Loss=0.9903\n",
            "  Batch 780: Loss=0.7883\n",
            "  Batch 790: Loss=1.0474\n",
            "  Batch 800: Loss=0.3555\n",
            "  Batch 810: Loss=0.5754\n",
            "  Batch 820: Loss=0.6547\n",
            "  Batch 830: Loss=0.9305\n",
            "  Batch 840: Loss=0.2465\n",
            "  Batch 850: Loss=1.3525\n",
            "  Batch 860: Loss=0.9527\n",
            "  Batch 870: Loss=0.7148\n",
            "  Batch 880: Loss=0.8124\n",
            "  Batch 890: Loss=0.7626\n",
            "  Batch 900: Loss=0.8216\n",
            "  Batch 910: Loss=0.7307\n",
            "  Batch 920: Loss=1.1496\n",
            "  Batch 930: Loss=0.5709\n",
            "  Batch 940: Loss=1.2624\n",
            "  Batch 950: Loss=0.7291\n",
            "  Batch 960: Loss=0.4787\n",
            "  Batch 970: Loss=0.1862\n",
            "  Batch 980: Loss=0.6948\n",
            "  Batch 990: Loss=0.4561\n",
            "  Batch 1000: Loss=2.8136\n",
            "  Batch 1010: Loss=0.8301\n",
            "  Batch 1020: Loss=0.6156\n",
            "  Batch 1030: Loss=1.0627\n",
            "\n",
            "  Evaluating...\n",
            "\n",
            "  Results: Train Loss=0.8542, Train Acc=0.6657, Test Acc=0.6168\n",
            "\n",
            "Epoch 9/15\n",
            "------------------------------------------------------------\n",
            "  Batch  10: Loss=0.6423\n",
            "  Batch  20: Loss=0.5709\n",
            "  Batch  30: Loss=2.4246\n",
            "  Batch  40: Loss=0.3590\n",
            "  Batch  50: Loss=1.7054\n",
            "  Batch  60: Loss=0.4635\n",
            "  Batch  70: Loss=0.7719\n",
            "  Batch  80: Loss=0.5185\n",
            "  Batch  90: Loss=2.9925\n",
            "  Batch 100: Loss=1.0951\n",
            "  Batch 110: Loss=0.4400\n",
            "  Batch 120: Loss=0.4613\n",
            "  Batch 130: Loss=0.8218\n",
            "  Batch 140: Loss=0.2047\n",
            "  Batch 150: Loss=0.6829\n",
            "  Batch 160: Loss=0.7529\n",
            "  Batch 170: Loss=1.9820\n",
            "  Batch 180: Loss=0.3079\n",
            "  Batch 190: Loss=0.1612\n",
            "  Batch 200: Loss=0.8239\n",
            "  Batch 210: Loss=0.8937\n",
            "  Batch 220: Loss=0.4459\n",
            "  Batch 230: Loss=2.0990\n",
            "  Batch 240: Loss=0.7966\n",
            "  Batch 250: Loss=0.8440\n",
            "  Batch 260: Loss=0.4050\n",
            "  Batch 270: Loss=0.7512\n",
            "  Batch 280: Loss=1.7209\n",
            "  Batch 290: Loss=0.5552\n",
            "  Batch 300: Loss=0.5204\n",
            "  Batch 310: Loss=0.2922\n",
            "  Batch 320: Loss=0.3471\n",
            "  Batch 330: Loss=0.7593\n",
            "  Batch 340: Loss=0.1102\n",
            "  Batch 350: Loss=0.8713\n",
            "  Batch 360: Loss=0.6341\n",
            "  Batch 370: Loss=0.0860\n",
            "  Batch 380: Loss=0.1478\n",
            "  Batch 390: Loss=0.6095\n",
            "  Batch 400: Loss=1.4996\n",
            "  Batch 410: Loss=1.8602\n",
            "  Batch 420: Loss=0.4293\n",
            "  Batch 430: Loss=0.9721\n",
            "  Batch 440: Loss=0.8694\n",
            "  Batch 450: Loss=1.5866\n",
            "  Batch 460: Loss=0.1679\n",
            "  Batch 470: Loss=0.3873\n",
            "  Batch 480: Loss=0.2373\n",
            "  Batch 490: Loss=0.4997\n",
            "  Batch 500: Loss=1.2374\n",
            "  Batch 510: Loss=1.6388\n",
            "  Batch 520: Loss=1.1144\n",
            "  Batch 530: Loss=0.3792\n",
            "  Batch 540: Loss=0.4630\n",
            "  Batch 550: Loss=0.8443\n",
            "  Batch 560: Loss=2.4426\n",
            "  Batch 570: Loss=1.7515\n",
            "  Batch 580: Loss=0.5521\n",
            "  Batch 590: Loss=2.1546\n",
            "  Batch 600: Loss=0.5488\n",
            "  Batch 610: Loss=0.9556\n",
            "  Batch 620: Loss=1.8113\n",
            "  Batch 630: Loss=0.9123\n",
            "  Batch 640: Loss=1.2162\n",
            "  Batch 650: Loss=1.0849\n",
            "  Batch 660: Loss=0.1459\n",
            "  Batch 670: Loss=1.0937\n",
            "  Batch 680: Loss=0.6791\n",
            "  Batch 690: Loss=0.5483\n",
            "  Batch 700: Loss=1.0978\n",
            "  Batch 710: Loss=0.9672\n",
            "  Batch 720: Loss=0.6267\n",
            "  Batch 730: Loss=0.2212\n",
            "  Batch 740: Loss=0.2676\n",
            "  Batch 750: Loss=1.0721\n",
            "  Batch 760: Loss=1.1036\n",
            "  Batch 770: Loss=0.3272\n",
            "  Batch 780: Loss=0.2275\n",
            "  Batch 790: Loss=0.3855\n",
            "  Batch 800: Loss=0.3331\n",
            "  Batch 810: Loss=0.3764\n",
            "  Batch 820: Loss=0.9498\n",
            "  Batch 830: Loss=0.5622\n",
            "  Batch 840: Loss=0.4457\n",
            "  Batch 850: Loss=1.0947\n",
            "  Batch 860: Loss=0.9040\n",
            "  Batch 870: Loss=0.7759\n",
            "  Batch 880: Loss=1.9484\n",
            "  Batch 890: Loss=1.4988\n",
            "  Batch 900: Loss=0.6938\n",
            "  Batch 910: Loss=1.0103\n",
            "  Batch 920: Loss=0.3262\n",
            "  Batch 930: Loss=1.0516\n",
            "  Batch 940: Loss=0.5954\n",
            "  Batch 950: Loss=0.4048\n",
            "  Batch 960: Loss=0.6071\n",
            "  Batch 970: Loss=0.3979\n",
            "  Batch 980: Loss=0.4896\n",
            "  Batch 990: Loss=1.0133\n",
            "  Batch 1000: Loss=0.5754\n",
            "  Batch 1010: Loss=0.7488\n",
            "  Batch 1020: Loss=1.8944\n",
            "  Batch 1030: Loss=0.2820\n",
            "\n",
            "  Evaluating...\n",
            "\n",
            "  Results: Train Loss=0.8247, Train Acc=0.6813, Test Acc=0.6201\n",
            "\n",
            "Epoch 10/15\n",
            "------------------------------------------------------------\n",
            "  Batch  10: Loss=1.5130\n",
            "  Batch  20: Loss=0.5464\n",
            "  Batch  30: Loss=0.5579\n",
            "  Batch  40: Loss=0.9428\n",
            "  Batch  50: Loss=0.4754\n",
            "  Batch  60: Loss=1.1047\n",
            "  Batch  70: Loss=0.4860\n",
            "  Batch  80: Loss=0.8534\n",
            "  Batch  90: Loss=1.0108\n",
            "  Batch 100: Loss=0.4880\n",
            "  Batch 110: Loss=1.9559\n",
            "  Batch 120: Loss=0.4294\n",
            "  Batch 130: Loss=1.0308\n",
            "  Batch 140: Loss=0.6555\n",
            "  Batch 150: Loss=1.2418\n",
            "  Batch 160: Loss=2.1740\n",
            "  Batch 170: Loss=0.6419\n",
            "  Batch 180: Loss=1.2857\n",
            "  Batch 190: Loss=1.4002\n",
            "  Batch 200: Loss=0.7298\n",
            "  Batch 210: Loss=1.0979\n",
            "  Batch 220: Loss=0.2839\n",
            "  Batch 230: Loss=0.4139\n",
            "  Batch 240: Loss=0.9823\n",
            "  Batch 250: Loss=0.6908\n",
            "  Batch 260: Loss=0.3308\n",
            "  Batch 270: Loss=0.1974\n",
            "  Batch 280: Loss=0.8835\n",
            "  Batch 290: Loss=1.0405\n",
            "  Batch 300: Loss=0.8176\n",
            "  Batch 310: Loss=1.7360\n",
            "  Batch 320: Loss=0.4344\n",
            "  Batch 330: Loss=0.8972\n",
            "  Batch 340: Loss=0.4052\n",
            "  Batch 350: Loss=1.8189\n",
            "  Batch 360: Loss=0.6125\n",
            "  Batch 370: Loss=1.0735\n",
            "  Batch 380: Loss=0.2300\n",
            "  Batch 390: Loss=0.2623\n",
            "  Batch 400: Loss=0.3320\n",
            "  Batch 410: Loss=1.4129\n",
            "  Batch 420: Loss=0.2012\n",
            "  Batch 430: Loss=0.8804\n",
            "  Batch 440: Loss=1.3694\n",
            "  Batch 450: Loss=0.5928\n",
            "  Batch 460: Loss=0.8179\n",
            "  Batch 470: Loss=0.9728\n",
            "  Batch 480: Loss=1.9986\n",
            "  Batch 490: Loss=0.5879\n",
            "  Batch 500: Loss=0.7897\n",
            "  Batch 510: Loss=0.8400\n",
            "  Batch 520: Loss=0.2192\n",
            "  Batch 530: Loss=1.8751\n",
            "  Batch 540: Loss=1.3002\n",
            "  Batch 550: Loss=0.3948\n",
            "  Batch 560: Loss=0.5189\n",
            "  Batch 570: Loss=1.0678\n",
            "  Batch 580: Loss=0.7966\n",
            "  Batch 590: Loss=0.8211\n",
            "  Batch 600: Loss=0.1625\n",
            "  Batch 610: Loss=0.6457\n",
            "  Batch 620: Loss=0.9566\n",
            "  Batch 630: Loss=0.3635\n",
            "  Batch 640: Loss=0.4874\n",
            "  Batch 650: Loss=0.6507\n",
            "  Batch 660: Loss=0.4271\n",
            "  Batch 670: Loss=0.2664\n",
            "  Batch 680: Loss=1.9584\n",
            "  Batch 690: Loss=0.3361\n",
            "  Batch 700: Loss=0.3173\n",
            "  Batch 710: Loss=0.7133\n",
            "  Batch 720: Loss=0.8087\n",
            "  Batch 730: Loss=0.5169\n",
            "  Batch 740: Loss=0.1358\n",
            "  Batch 750: Loss=0.9251\n",
            "  Batch 760: Loss=2.0686\n",
            "  Batch 770: Loss=0.1379\n",
            "  Batch 780: Loss=0.3555\n",
            "  Batch 790: Loss=0.2643\n",
            "  Batch 800: Loss=0.3207\n",
            "  Batch 810: Loss=1.0177\n",
            "  Batch 820: Loss=0.9903\n",
            "  Batch 830: Loss=1.1266\n",
            "  Batch 840: Loss=1.2450\n",
            "  Batch 850: Loss=0.1877\n",
            "  Batch 860: Loss=0.6036\n",
            "  Batch 870: Loss=0.7506\n",
            "  Batch 880: Loss=0.5118\n",
            "  Batch 890: Loss=1.9610\n",
            "  Batch 900: Loss=0.6924\n",
            "  Batch 910: Loss=0.4251\n",
            "  Batch 920: Loss=0.2299\n",
            "  Batch 930: Loss=0.5211\n",
            "  Batch 940: Loss=0.3639\n",
            "  Batch 950: Loss=1.4489\n",
            "  Batch 960: Loss=2.0461\n",
            "  Batch 970: Loss=0.3907\n",
            "  Batch 980: Loss=0.9696\n",
            "  Batch 990: Loss=1.4771\n",
            "  Batch 1000: Loss=0.8353\n",
            "  Batch 1010: Loss=0.0573\n",
            "  Batch 1020: Loss=1.0136\n",
            "  Batch 1030: Loss=0.1563\n",
            "\n",
            "  Evaluating...\n",
            "\n",
            "  Results: Train Loss=0.7698, Train Acc=0.7054, Test Acc=0.6287\n",
            "\n",
            "Epoch 11/15\n",
            "------------------------------------------------------------\n",
            "  Batch  10: Loss=0.1614\n",
            "  Batch  20: Loss=0.9286\n",
            "  Batch  30: Loss=0.3152\n",
            "  Batch  40: Loss=0.7387\n",
            "  Batch  50: Loss=0.0954\n",
            "  Batch  60: Loss=0.5706\n",
            "  Batch  70: Loss=0.3534\n",
            "  Batch  80: Loss=0.6131\n",
            "  Batch  90: Loss=1.3646\n",
            "  Batch 100: Loss=1.1721\n",
            "  Batch 110: Loss=1.4416\n",
            "  Batch 120: Loss=0.2593\n",
            "  Batch 130: Loss=0.9579\n",
            "  Batch 140: Loss=0.5240\n",
            "  Batch 150: Loss=0.9867\n",
            "  Batch 160: Loss=0.3057\n",
            "  Batch 170: Loss=0.1513\n",
            "  Batch 180: Loss=0.6061\n",
            "  Batch 190: Loss=0.7989\n",
            "  Batch 200: Loss=2.5114\n",
            "  Batch 210: Loss=0.4795\n",
            "  Batch 220: Loss=0.3089\n",
            "  Batch 230: Loss=0.7825\n",
            "  Batch 240: Loss=0.2916\n",
            "  Batch 250: Loss=0.5068\n",
            "  Batch 260: Loss=0.9601\n",
            "  Batch 270: Loss=0.5115\n",
            "  Batch 280: Loss=0.5166\n",
            "  Batch 290: Loss=2.7591\n",
            "  Batch 300: Loss=0.5105\n",
            "  Batch 310: Loss=0.0646\n",
            "  Batch 320: Loss=0.9135\n",
            "  Batch 330: Loss=0.3780\n",
            "  Batch 340: Loss=1.0611\n",
            "  Batch 350: Loss=0.6781\n",
            "  Batch 360: Loss=1.3535\n",
            "  Batch 370: Loss=0.3524\n",
            "  Batch 380: Loss=0.9281\n",
            "  Batch 390: Loss=0.6722\n",
            "  Batch 400: Loss=0.4804\n",
            "  Batch 410: Loss=1.3535\n",
            "  Batch 420: Loss=1.2783\n",
            "  Batch 430: Loss=0.4936\n",
            "  Batch 440: Loss=0.4349\n",
            "  Batch 450: Loss=1.0725\n",
            "  Batch 460: Loss=0.3422\n",
            "  Batch 470: Loss=0.3882\n",
            "  Batch 480: Loss=0.3480\n",
            "  Batch 490: Loss=0.4197\n",
            "  Batch 500: Loss=0.4559\n",
            "  Batch 510: Loss=1.0384\n",
            "  Batch 520: Loss=1.3576\n",
            "  Batch 530: Loss=0.3679\n",
            "  Batch 540: Loss=0.7191\n",
            "  Batch 550: Loss=0.2848\n",
            "  Batch 560: Loss=0.5692\n",
            "  Batch 570: Loss=0.2367\n",
            "  Batch 580: Loss=0.2251\n",
            "  Batch 590: Loss=0.4381\n",
            "  Batch 600: Loss=0.3547\n",
            "  Batch 610: Loss=2.2923\n",
            "  Batch 620: Loss=0.3806\n",
            "  Batch 630: Loss=0.7299\n",
            "  Batch 640: Loss=0.1883\n",
            "  Batch 650: Loss=0.9177\n",
            "  Batch 660: Loss=0.2816\n",
            "  Batch 670: Loss=0.4251\n",
            "  Batch 680: Loss=0.2588\n",
            "  Batch 690: Loss=0.9577\n",
            "  Batch 700: Loss=0.2963\n",
            "  Batch 710: Loss=1.2460\n",
            "  Batch 720: Loss=0.2214\n",
            "  Batch 730: Loss=2.3186\n",
            "  Batch 740: Loss=0.3752\n",
            "  Batch 750: Loss=1.1020\n",
            "  Batch 760: Loss=0.7872\n",
            "  Batch 770: Loss=1.2415\n",
            "  Batch 780: Loss=0.3455\n",
            "  Batch 790: Loss=1.8454\n",
            "  Batch 800: Loss=0.8154\n",
            "  Batch 810: Loss=0.3560\n",
            "  Batch 820: Loss=0.5740\n",
            "  Batch 830: Loss=1.0435\n",
            "  Batch 840: Loss=0.8072\n",
            "  Batch 850: Loss=0.4770\n",
            "  Batch 860: Loss=0.2749\n",
            "  Batch 870: Loss=1.0527\n",
            "  Batch 880: Loss=0.6198\n",
            "  Batch 890: Loss=0.0876\n",
            "  Batch 900: Loss=0.1953\n",
            "  Batch 910: Loss=2.7570\n",
            "  Batch 920: Loss=0.5336\n",
            "  Batch 930: Loss=0.8286\n",
            "  Batch 940: Loss=0.8629\n",
            "  Batch 950: Loss=0.8761\n",
            "  Batch 960: Loss=0.8440\n",
            "  Batch 970: Loss=1.6224\n",
            "  Batch 980: Loss=0.2049\n",
            "  Batch 990: Loss=0.2944\n",
            "  Batch 1000: Loss=0.3195\n",
            "  Batch 1010: Loss=0.2878\n",
            "  Batch 1020: Loss=0.6068\n",
            "  Batch 1030: Loss=0.6143\n",
            "\n",
            "  Evaluating...\n",
            "\n",
            "  Results: Train Loss=0.7704, Train Acc=0.7008, Test Acc=0.6486\n",
            "\n",
            "Epoch 12/15\n",
            "------------------------------------------------------------\n",
            "  Batch  10: Loss=0.3121\n",
            "  Batch  20: Loss=0.3282\n",
            "  Batch  30: Loss=0.1514\n",
            "  Batch  40: Loss=0.8407\n",
            "  Batch  50: Loss=0.7649\n",
            "  Batch  60: Loss=0.2093\n",
            "  Batch  70: Loss=0.8766\n",
            "  Batch  80: Loss=0.8550\n",
            "  Batch  90: Loss=0.2219\n",
            "  Batch 100: Loss=0.4488\n",
            "  Batch 110: Loss=0.5639\n",
            "  Batch 120: Loss=0.7630\n",
            "  Batch 130: Loss=0.3604\n",
            "  Batch 140: Loss=1.3007\n",
            "  Batch 150: Loss=0.7523\n",
            "  Batch 160: Loss=0.1947\n",
            "  Batch 170: Loss=0.6465\n",
            "  Batch 180: Loss=1.2467\n",
            "  Batch 190: Loss=0.7531\n",
            "  Batch 200: Loss=0.3332\n",
            "  Batch 210: Loss=1.5357\n",
            "  Batch 220: Loss=0.1436\n",
            "  Batch 230: Loss=1.6885\n",
            "  Batch 240: Loss=0.9496\n",
            "  Batch 250: Loss=1.9590\n",
            "  Batch 260: Loss=1.2104\n",
            "  Batch 270: Loss=0.8243\n",
            "  Batch 280: Loss=0.0992\n",
            "  Batch 290: Loss=0.2973\n",
            "  Batch 300: Loss=0.3635\n",
            "  Batch 310: Loss=0.2069\n",
            "  Batch 320: Loss=0.7677\n",
            "  Batch 330: Loss=0.4607\n",
            "  Batch 340: Loss=0.1480\n",
            "  Batch 350: Loss=0.5215\n",
            "  Batch 360: Loss=0.4828\n",
            "  Batch 370: Loss=0.1805\n",
            "  Batch 380: Loss=1.5857\n",
            "  Batch 390: Loss=0.4201\n",
            "  Batch 400: Loss=0.2812\n",
            "  Batch 410: Loss=0.1594\n",
            "  Batch 420: Loss=0.7728\n",
            "  Batch 430: Loss=0.8572\n",
            "  Batch 440: Loss=1.2048\n",
            "  Batch 450: Loss=1.0480\n",
            "  Batch 460: Loss=0.1738\n",
            "  Batch 470: Loss=1.7352\n",
            "  Batch 480: Loss=0.3797\n",
            "  Batch 490: Loss=0.7672\n",
            "  Batch 500: Loss=0.9187\n",
            "  Batch 510: Loss=0.1181\n",
            "  Batch 520: Loss=0.1993\n",
            "  Batch 530: Loss=0.5187\n",
            "  Batch 540: Loss=0.1433\n",
            "  Batch 550: Loss=1.0549\n",
            "  Batch 560: Loss=0.3220\n",
            "  Batch 570: Loss=0.9346\n",
            "  Batch 580: Loss=1.8472\n",
            "  Batch 590: Loss=0.9084\n",
            "  Batch 600: Loss=0.1724\n",
            "  Batch 610: Loss=0.1193\n",
            "  Batch 620: Loss=1.0442\n",
            "  Batch 630: Loss=1.7328\n",
            "  Batch 640: Loss=0.1670\n",
            "  Batch 650: Loss=0.2554\n",
            "  Batch 660: Loss=0.1735\n",
            "  Batch 670: Loss=1.9488\n",
            "  Batch 680: Loss=0.0461\n",
            "  Batch 690: Loss=1.5617\n",
            "  Batch 700: Loss=0.3548\n",
            "  Batch 710: Loss=0.5378\n",
            "  Batch 720: Loss=0.3288\n",
            "  Batch 730: Loss=0.6207\n",
            "  Batch 740: Loss=0.3271\n",
            "  Batch 750: Loss=0.3655\n",
            "  Batch 760: Loss=1.6018\n",
            "  Batch 770: Loss=0.7125\n",
            "  Batch 780: Loss=0.1630\n",
            "  Batch 790: Loss=0.5037\n",
            "  Batch 800: Loss=1.1187\n",
            "  Batch 810: Loss=0.3765\n",
            "  Batch 820: Loss=0.2531\n",
            "  Batch 830: Loss=2.9894\n",
            "  Batch 840: Loss=1.4713\n",
            "  Batch 850: Loss=0.2622\n",
            "  Batch 860: Loss=0.4067\n",
            "  Batch 870: Loss=0.9125\n",
            "  Batch 880: Loss=0.4379\n",
            "  Batch 890: Loss=0.4119\n",
            "  Batch 900: Loss=0.4410\n",
            "  Batch 910: Loss=1.3976\n",
            "  Batch 920: Loss=0.1114\n",
            "  Batch 930: Loss=0.7640\n",
            "  Batch 940: Loss=1.2001\n",
            "  Batch 950: Loss=0.1977\n",
            "  Batch 960: Loss=0.3637\n",
            "  Batch 970: Loss=1.0286\n",
            "  Batch 980: Loss=0.6728\n",
            "  Batch 990: Loss=0.2033\n",
            "  Batch 1000: Loss=0.4150\n",
            "  Batch 1010: Loss=0.8022\n",
            "  Batch 1020: Loss=0.2630\n",
            "  Batch 1030: Loss=0.1705\n",
            "\n",
            "  Evaluating...\n",
            "\n",
            "  Results: Train Loss=0.7391, Train Acc=0.7153, Test Acc=0.5989\n",
            "\n",
            "Epoch 13/15\n",
            "------------------------------------------------------------\n",
            "  Batch  10: Loss=0.3547\n",
            "  Batch  20: Loss=0.3903\n",
            "  Batch  30: Loss=1.5295\n",
            "  Batch  40: Loss=1.0847\n",
            "  Batch  50: Loss=1.0596\n",
            "  Batch  60: Loss=0.8836\n",
            "  Batch  70: Loss=0.2271\n",
            "  Batch  80: Loss=0.7257\n",
            "  Batch  90: Loss=1.3039\n",
            "  Batch 100: Loss=0.8247\n",
            "  Batch 110: Loss=0.8226\n",
            "  Batch 120: Loss=2.6418\n",
            "  Batch 130: Loss=0.6650\n",
            "  Batch 140: Loss=2.1404\n",
            "  Batch 150: Loss=0.5605\n",
            "  Batch 160: Loss=0.4906\n",
            "  Batch 170: Loss=0.4456\n",
            "  Batch 180: Loss=1.9312\n",
            "  Batch 190: Loss=0.3572\n",
            "  Batch 200: Loss=2.4019\n",
            "  Batch 210: Loss=0.4100\n",
            "  Batch 220: Loss=0.4125\n",
            "  Batch 230: Loss=0.9846\n",
            "  Batch 240: Loss=0.3373\n",
            "  Batch 250: Loss=0.7417\n",
            "  Batch 260: Loss=0.8563\n",
            "  Batch 270: Loss=0.1074\n",
            "  Batch 280: Loss=0.1738\n",
            "  Batch 290: Loss=0.1998\n",
            "  Batch 300: Loss=0.4627\n",
            "  Batch 310: Loss=0.8035\n",
            "  Batch 320: Loss=1.0331\n",
            "  Batch 330: Loss=0.2504\n",
            "  Batch 340: Loss=1.3525\n",
            "  Batch 350: Loss=0.9080\n",
            "  Batch 360: Loss=0.4050\n",
            "  Batch 370: Loss=0.1767\n",
            "  Batch 380: Loss=0.7796\n",
            "  Batch 390: Loss=0.9015\n",
            "  Batch 400: Loss=0.4304\n",
            "  Batch 410: Loss=0.4933\n",
            "  Batch 420: Loss=0.4979\n",
            "  Batch 430: Loss=0.2156\n",
            "  Batch 440: Loss=0.1186\n",
            "  Batch 450: Loss=0.5045\n",
            "  Batch 460: Loss=0.0485\n",
            "  Batch 470: Loss=0.7093\n",
            "  Batch 480: Loss=1.4155\n",
            "  Batch 490: Loss=0.1020\n",
            "  Batch 500: Loss=0.8122\n",
            "  Batch 510: Loss=0.2628\n",
            "  Batch 520: Loss=2.0866\n",
            "  Batch 530: Loss=0.9622\n",
            "  Batch 540: Loss=0.5456\n",
            "  Batch 550: Loss=0.1112\n",
            "  Batch 560: Loss=0.8721\n",
            "  Batch 570: Loss=1.1418\n",
            "  Batch 580: Loss=2.6854\n",
            "  Batch 590: Loss=0.7238\n",
            "  Batch 600: Loss=0.9454\n",
            "  Batch 610: Loss=1.3325\n",
            "  Batch 620: Loss=0.1934\n",
            "  Batch 630: Loss=0.5771\n",
            "  Batch 640: Loss=0.7829\n",
            "  Batch 650: Loss=0.7663\n",
            "  Batch 660: Loss=0.2166\n",
            "  Batch 670: Loss=1.5797\n",
            "  Batch 680: Loss=0.5194\n",
            "  Batch 690: Loss=0.9848\n",
            "  Batch 700: Loss=1.4662\n",
            "  Batch 710: Loss=0.2859\n",
            "  Batch 720: Loss=0.5118\n",
            "  Batch 730: Loss=0.6745\n",
            "  Batch 740: Loss=0.4485\n",
            "  Batch 750: Loss=0.4193\n",
            "  Batch 760: Loss=0.5405\n",
            "  Batch 770: Loss=0.5154\n",
            "  Batch 780: Loss=1.2568\n",
            "  Batch 790: Loss=0.4880\n",
            "  Batch 800: Loss=0.4191\n",
            "  Batch 810: Loss=0.6639\n",
            "  Batch 820: Loss=0.7453\n",
            "  Batch 830: Loss=0.5768\n",
            "  Batch 840: Loss=1.3821\n",
            "  Batch 850: Loss=1.8260\n",
            "  Batch 860: Loss=1.0614\n",
            "  Batch 870: Loss=1.3925\n",
            "  Batch 880: Loss=0.3821\n",
            "  Batch 890: Loss=0.0913\n",
            "  Batch 900: Loss=0.4512\n",
            "  Batch 910: Loss=0.0518\n",
            "  Batch 920: Loss=0.3760\n",
            "  Batch 930: Loss=0.3326\n",
            "  Batch 940: Loss=0.2615\n",
            "  Batch 950: Loss=0.3663\n",
            "  Batch 960: Loss=0.4659\n",
            "  Batch 970: Loss=0.1946\n",
            "  Batch 980: Loss=1.5942\n",
            "  Batch 990: Loss=0.5395\n",
            "  Batch 1000: Loss=0.4957\n",
            "  Batch 1010: Loss=0.2532\n",
            "  Batch 1020: Loss=0.2027\n",
            "  Batch 1030: Loss=0.0641\n",
            "\n",
            "  Evaluating...\n",
            "\n",
            "  Results: Train Loss=0.7454, Train Acc=0.7066, Test Acc=0.5705\n",
            "\n",
            "Epoch 14/15\n",
            "------------------------------------------------------------\n",
            "  Batch  10: Loss=0.3147\n",
            "  Batch  20: Loss=0.3547\n",
            "  Batch  30: Loss=0.2861\n",
            "  Batch  40: Loss=0.6499\n",
            "  Batch  50: Loss=0.2717\n",
            "  Batch  60: Loss=0.8494\n",
            "  Batch  70: Loss=0.8675\n",
            "  Batch  80: Loss=1.1098\n",
            "  Batch  90: Loss=0.1151\n",
            "  Batch 100: Loss=0.3782\n",
            "  Batch 110: Loss=0.7401\n",
            "  Batch 120: Loss=0.0799\n",
            "  Batch 130: Loss=0.0662\n",
            "  Batch 140: Loss=0.8136\n",
            "  Batch 150: Loss=1.3098\n",
            "  Batch 160: Loss=1.8086\n",
            "  Batch 170: Loss=2.0559\n",
            "  Batch 180: Loss=0.7993\n",
            "  Batch 190: Loss=0.6473\n",
            "  Batch 200: Loss=0.6436\n",
            "  Batch 210: Loss=0.4662\n",
            "  Batch 220: Loss=0.2507\n",
            "  Batch 230: Loss=1.4472\n",
            "  Batch 240: Loss=0.0861\n",
            "  Batch 250: Loss=0.7021\n",
            "  Batch 260: Loss=1.5048\n",
            "  Batch 270: Loss=0.0998\n",
            "  Batch 280: Loss=0.7264\n",
            "  Batch 290: Loss=0.3982\n",
            "  Batch 300: Loss=0.4562\n",
            "  Batch 310: Loss=0.1495\n",
            "  Batch 320: Loss=0.4503\n",
            "  Batch 330: Loss=0.3567\n",
            "  Batch 340: Loss=0.2909\n",
            "  Batch 350: Loss=0.6579\n",
            "  Batch 360: Loss=0.0651\n",
            "  Batch 370: Loss=0.7769\n",
            "  Batch 380: Loss=0.6078\n",
            "  Batch 390: Loss=1.2490\n",
            "  Batch 400: Loss=0.6312\n",
            "  Batch 410: Loss=2.1126\n",
            "  Batch 420: Loss=0.9708\n",
            "  Batch 430: Loss=0.5627\n",
            "  Batch 440: Loss=0.3098\n",
            "  Batch 450: Loss=0.2819\n",
            "  Batch 460: Loss=0.4313\n",
            "  Batch 470: Loss=0.8004\n",
            "  Batch 480: Loss=0.9105\n",
            "  Batch 490: Loss=0.1374\n",
            "  Batch 500: Loss=1.3273\n",
            "  Batch 510: Loss=0.2824\n",
            "  Batch 520: Loss=0.7310\n",
            "  Batch 530: Loss=0.3073\n",
            "  Batch 540: Loss=1.2266\n",
            "  Batch 550: Loss=1.2469\n",
            "  Batch 560: Loss=0.6079\n",
            "  Batch 570: Loss=1.1221\n",
            "  Batch 580: Loss=0.1629\n",
            "  Batch 590: Loss=0.7603\n",
            "  Batch 600: Loss=0.5290\n",
            "  Batch 610: Loss=0.7884\n",
            "  Batch 620: Loss=0.3171\n",
            "  Batch 630: Loss=2.2005\n",
            "  Batch 640: Loss=1.2182\n",
            "  Batch 650: Loss=0.7296\n",
            "  Batch 660: Loss=0.1280\n",
            "  Batch 670: Loss=0.1750\n",
            "  Batch 680: Loss=0.2374\n",
            "  Batch 690: Loss=0.1417\n",
            "  Batch 700: Loss=1.0341\n",
            "  Batch 710: Loss=2.5055\n",
            "  Batch 720: Loss=0.5410\n",
            "  Batch 730: Loss=0.1500\n",
            "  Batch 740: Loss=1.4248\n",
            "  Batch 750: Loss=0.7791\n",
            "  Batch 760: Loss=0.3090\n",
            "  Batch 770: Loss=1.6126\n",
            "  Batch 780: Loss=0.8319\n",
            "  Batch 790: Loss=1.0907\n",
            "  Batch 800: Loss=0.9382\n",
            "  Batch 810: Loss=0.9044\n",
            "  Batch 820: Loss=2.3759\n",
            "  Batch 830: Loss=1.4564\n",
            "  Batch 840: Loss=0.0841\n",
            "  Batch 850: Loss=0.1342\n",
            "  Batch 860: Loss=0.5968\n",
            "  Batch 870: Loss=0.6839\n",
            "  Batch 880: Loss=0.3128\n",
            "  Batch 890: Loss=0.7485\n",
            "  Batch 900: Loss=0.2543\n",
            "  Batch 910: Loss=1.7878\n",
            "  Batch 920: Loss=0.8014\n",
            "  Batch 930: Loss=1.2098\n",
            "  Batch 940: Loss=0.2674\n",
            "  Batch 950: Loss=0.8735\n",
            "  Batch 960: Loss=0.4217\n",
            "  Batch 970: Loss=1.9041\n",
            "  Batch 980: Loss=0.2343\n",
            "  Batch 990: Loss=1.5169\n",
            "  Batch 1000: Loss=0.5561\n",
            "  Batch 1010: Loss=0.4534\n",
            "  Batch 1020: Loss=0.6690\n",
            "  Batch 1030: Loss=0.0633\n",
            "\n",
            "  Evaluating...\n",
            "\n",
            "  Results: Train Loss=0.7367, Train Acc=0.7134, Test Acc=0.5943\n",
            "\n",
            "Epoch 15/15\n",
            "------------------------------------------------------------\n",
            "  Batch  10: Loss=0.9186\n",
            "  Batch  20: Loss=0.1661\n",
            "  Batch  30: Loss=0.0879\n",
            "  Batch  40: Loss=0.3635\n",
            "  Batch  50: Loss=0.5095\n",
            "  Batch  60: Loss=1.8372\n",
            "  Batch  70: Loss=0.0552\n",
            "  Batch  80: Loss=0.9660\n",
            "  Batch  90: Loss=0.7147\n",
            "  Batch 100: Loss=0.3450\n",
            "  Batch 110: Loss=0.2364\n",
            "  Batch 120: Loss=0.4985\n",
            "  Batch 130: Loss=1.9767\n",
            "  Batch 140: Loss=1.2676\n",
            "  Batch 150: Loss=1.2546\n",
            "  Batch 160: Loss=0.4733\n",
            "  Batch 170: Loss=0.6047\n",
            "  Batch 180: Loss=0.4935\n",
            "  Batch 190: Loss=0.1650\n",
            "  Batch 200: Loss=0.3300\n",
            "  Batch 210: Loss=0.2840\n",
            "  Batch 220: Loss=1.0489\n",
            "  Batch 230: Loss=2.2574\n",
            "  Batch 240: Loss=0.5103\n",
            "  Batch 250: Loss=1.7468\n",
            "  Batch 260: Loss=0.0907\n",
            "  Batch 270: Loss=0.1717\n",
            "  Batch 280: Loss=0.5629\n",
            "  Batch 290: Loss=0.2280\n",
            "  Batch 300: Loss=0.4670\n",
            "  Batch 310: Loss=0.0841\n",
            "  Batch 320: Loss=1.1016\n",
            "  Batch 330: Loss=0.8186\n",
            "  Batch 340: Loss=2.3903\n",
            "  Batch 350: Loss=1.2442\n",
            "  Batch 360: Loss=0.1051\n",
            "  Batch 370: Loss=0.7014\n",
            "  Batch 380: Loss=0.3993\n",
            "  Batch 390: Loss=0.3037\n",
            "  Batch 400: Loss=0.7474\n",
            "  Batch 410: Loss=0.9630\n",
            "  Batch 420: Loss=0.5806\n",
            "  Batch 430: Loss=0.2907\n",
            "  Batch 440: Loss=0.1363\n",
            "  Batch 450: Loss=0.0882\n",
            "  Batch 460: Loss=0.5333\n",
            "  Batch 470: Loss=0.7732\n",
            "  Batch 480: Loss=0.3859\n",
            "  Batch 490: Loss=0.2074\n",
            "  Batch 500: Loss=0.6666\n",
            "  Batch 510: Loss=1.0840\n",
            "  Batch 520: Loss=1.8340\n",
            "  Batch 530: Loss=0.1343\n",
            "  Batch 540: Loss=0.7153\n",
            "  Batch 550: Loss=0.6004\n",
            "  Batch 560: Loss=1.8619\n",
            "  Batch 570: Loss=0.3756\n",
            "  Batch 580: Loss=0.6304\n",
            "  Batch 590: Loss=0.4310\n",
            "  Batch 600: Loss=0.2580\n",
            "  Batch 610: Loss=1.1747\n",
            "  Batch 620: Loss=0.2453\n",
            "  Batch 630: Loss=0.3600\n",
            "  Batch 640: Loss=0.3320\n",
            "  Batch 650: Loss=0.2689\n",
            "  Batch 660: Loss=0.1401\n",
            "  Batch 670: Loss=0.4288\n",
            "  Batch 680: Loss=0.4715\n",
            "  Batch 690: Loss=0.5260\n",
            "  Batch 700: Loss=0.9517\n",
            "  Batch 710: Loss=1.6729\n",
            "  Batch 720: Loss=0.4731\n",
            "  Batch 730: Loss=0.3677\n",
            "  Batch 740: Loss=0.6586\n",
            "  Batch 750: Loss=0.3329\n",
            "  Batch 760: Loss=1.3219\n",
            "  Batch 770: Loss=0.2468\n",
            "  Batch 780: Loss=0.7783\n",
            "  Batch 790: Loss=0.3080\n",
            "  Batch 800: Loss=1.2186\n",
            "  Batch 810: Loss=0.0598\n",
            "  Batch 820: Loss=0.5175\n",
            "  Batch 830: Loss=0.6083\n",
            "  Batch 840: Loss=1.4234\n",
            "  Batch 850: Loss=0.2602\n",
            "  Batch 860: Loss=0.4937\n",
            "  Batch 870: Loss=0.9604\n",
            "  Batch 880: Loss=0.2981\n",
            "  Batch 890: Loss=0.6356\n",
            "  Batch 900: Loss=0.1166\n",
            "  Batch 910: Loss=0.3719\n",
            "  Batch 920: Loss=1.0174\n",
            "  Batch 930: Loss=2.5869\n",
            "  Batch 940: Loss=0.5465\n",
            "  Batch 950: Loss=0.6617\n",
            "  Batch 960: Loss=0.3843\n",
            "  Batch 970: Loss=0.2153\n",
            "  Batch 980: Loss=2.0880\n",
            "  Batch 990: Loss=0.1842\n",
            "  Batch 1000: Loss=0.9279\n",
            "  Batch 1010: Loss=0.9143\n",
            "  Batch 1020: Loss=1.3916\n",
            "  Batch 1030: Loss=0.8412\n",
            "\n",
            "  Evaluating...\n",
            "\n",
            "  Results: Train Loss=0.6923, Train Acc=0.7439, Test Acc=0.6254\n",
            "\n",
            "============================================================\n",
            "Training Complete! Best Test Acc: 0.6499\n",
            "============================================================\n",
            "\n",
            "Saving...\n",
            "Saved to /content/drive/MyDrive/POC_Dataset/results\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot saved!\n",
            "\n",
            "Complete\n"
          ]
        }
      ]
    }
  ]
}